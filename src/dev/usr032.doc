=IGN
********************************************************************************
usr031.doc: this file is part of the PPXpp system

Copyright (c) 2002 Lemma 1 Ltd.

See the file LICENSE for your rights to use and change this file.

Contact: Rob Arthan < rda@lemma-one.com >
********************************************************************************
=TEX
%
% This document includes some xfig diagrams usr032?.fig..
%
%%%%% YOU MAY WANT TO CHANGE POINT SIZE IN THE FOLLOWING:
\documentclass[a4paper,12pt]{article}

%%%%% YOU CAN ADD OTHER PACKAGES AS NEEDED BELOW:
\usepackage{A4}
\usepackage{Lemma1}
\usepackage{ProofPower}
\usepackage{epsf}
\usepackage{float}

\def\SLRP{{\sf SLRP}}
\def\Slrp{{\tt slrp}}

\def\FigRef#1{\ref{#1} on page \pageref{#1}}
\def\Hide#1{\relax}

\vertbarfalse


\newenvironment{NoBoxFig}[3][htp]{%
\begin{figure}[#1]
\def\Label{\label{#3}}
\def\Caption{\caption{#2}}
\begin{center}
\begin{tabular}{c}
\begin{minipage}{.85\hsize}
}{%
\end{minipage}
\end{tabular}
\end{center}
\Caption\Label
\end{figure}}

\newenvironment{Fig}[3][htp]{%
\begin{figure}[#1]
\def\Label{\label{#3}}
\def\Caption{\caption{#2}}
\begin{center}
\begin{tabular}{|c|}\hline
\begin{minipage}{.85\hsize}
}{%
\end{minipage}\\\hline
\end{tabular}
\end{center}
\Caption\Label
\end{figure}}

%%%%% YOU WILL USUALLY WANT TO CHANGE THE FOLLOWING TO SUIT YOU AND YOUR DOCUMENT:

\def\Title{ {\SLRP} User Guide }

\def\Abstract{\begin{center}
{\bf Abstract}\par\parbox{0.7\hsize}
{\small This document is the user guide for {\SLRP} --- a simple parser generator for Standard ML.}
\end{center}}

\def\Reference{LEMMA1/XPP/USR031}

\def\Author{R.D. Arthan}

\def\EMail{{\tt rda@lemma-one.com}}

\def\Phone{+44 118 958 4409}

\def\Fax{+44 118 956 1920}

%%%%% YOU MAY WANT TO CHANGE THE FOLLOWING TO GET A NICE FRONT PAGE:
\def\FrontPageTitle{ {\huge ProofPower\\---\\{\SLRP} User Guide } }
\def\FrontPageHeader{\raisebox{16ex}{\begin{tabular}[t]{c}
\bf Copyright \copyright\ : Lemma 1 Ltd \number\year\\\strut\\
\end{tabular}}}
\begin{centering}



\end{centering}

%%%%% THE FOLLOWING DEFAULTS WILL GENERALLY BE RIGHT:

\def\Version{$Revision: 1.4 $%
}
\def\Date{\FormatDate{$Date: 2003/03/09 22:07:46 $%
}}

%%%%% NOW BEGIN THE DOCUMENT AND MAKE THE FRONT PAGE

\begin{document}
\headsep=0mm
\FrontPage
\headsep=10mm

%%%%% STANDARD RED-TAPE SECTIONS (MAY WANT TO INTERLEAVE SOME \newpage COMMANDS IN THESE)

%%%%% CONTENTS:

\subsection{Contents}

\tableofcontents
\newpage
\subsection{List of Figures}
\listoffigures
%%%%% REFERENCES:

\subsection{References}

\bibliographystyle{fmu}

%%%%% CHANGE THE FOLLOWING AS NECESSARY (E.G., TO PICK UP daz.bib):
{\raggedright
\bibliography{fmu}
}
%%%%% CHANGES HISTORY:
\subsection{Changes History}
\begin{description}
\item[Issues 1.1 -- 1.n] Author's initial drafts.
\end{description}


%%%%%  CHANGES FORECAST:

\subsection{Changes Forecast}

None at this release.

%%%%% DISTRIBUTION LIST

%\subsection{Distribution}
%\begin{center}
%\begin{tabular}{ll}
%Rob Arthan & Lemma 1\\
%Gill Prout & Home \\
%Roger Jones & Home
%\end{tabular}
%\end{center}


\newpage

%%%%% NOW THE CREATIVE BIT:

\section{INTRODUCTION}

{\SLRP} is a simple but powerful parser generator for Standard ML. The input to {\SLRP}
is a grammar written in a version of Backus-Naur Format (BNF), \cite{BS6154}.
The output from {\SLRP} is a file of Standard ML code that can be used to construct
a parser from the language specified by the grammar.
Grammar rules may include Standard ML action code to be executed when the rule is applied
during a parse.
{\SLRP} can also, optionally, output a listing of the grammar and details of its analysis
of the grammar to assist you in designing the language or the grammar that defines it.

{\SLRP} is similar in conception to the widely used parser generators like {\tt yacc} and {\tt bison}
for C and ML-Yacc for Standard ML. {\SLRP} differs from these in that the parser it generates
is table-driven rather than comprising a mixture of tables and decision-making code. The tables
are interpreted by a simple polymorphic parser-driver function that takes as its arguments
functions that you supply to carry out application-specific tasks such as reading the input stream
and reporting errors. The organisation of the parser code is easy to understand and could
easily be modified, e.g., to work with a different dialect of ML.

{\SLRP} does not include an automatic lexical analyser generator. It does support production
of a generic parser from a grammar. This means that {\SLRP}  can automatically
generate the action code and the supporting functions for you.  The result is a complete
working parser whose input is a sequence of terminal symbols in the same format as was
used in the grammar and whose output is a parse tree. This provides a framework which
you can readily adapt to provide the parsing functionality that your application requires.

{\SLRP} is currently available to run with the Poly/ML or Standard ML of New Jersey compilers.
The output from {\SLRP} is Standard ML code that should work with any Standard ML compiler.
{\SLRP} includes the {\Product} library of utility functions and the
code it generates does depend on these. However, substitutes for the small repertoire
of functions actually used (primarily concerned with table lookup) could easily be supplied
to work in other environments.

The {\SLRP} source code and documentation is packaged using. PPTex, the {\Product}
document preparation and literate programming system. You do not need to use
PPTex to use {\SLRP}, but it does provide a convenient way of packaging the various
source files and scripts you need to build a parser with {\SLRP}.

Like {\tt yacc} and {\tt bison}, {\SLRP} implements a version of the LALR(1)  algorithm. This
means that it is capable of generating parsers for a wide range of practical languages.
The {\SLRP} distribution includes example grammars for the programming languages
Ada 95,  Java 1.1 and Pascal and a complete working parser for C. {\SLRP} is also
used for the parsers for the object languages HOL and Z supported by {\Product}.

\section{GETTING STARTED WITH \SLRP}\label{STARTING}

The first task in using {\SLRP} to build a parser is to prepare a text file containing
a context-free grammar for the language you want to parse. The file should be given a name ending
in ``{\tt .txt}''.  The format of this file is specified in detail in appendix~\ref{INPUTFORMAT}.
Figure~\ref{usr032a.grm.txt} shows an example file containing a grammar for a language of arithmetic expressions:

\begin{Fig}[h]{A Grammar for Arithmetic Expressions}{usr032a.grm.txt}
=DUMP usr032a.grm.txt
Expression =	Sum;

Atom =		literal
		|	identifier
		|	`(`, Expression, `)`;

Application =	Atom
		|	`-`, Application
		|	`+`, Application
		|	identifier, `(`, Expression, `)`;

Product =		Application
		|	Product, `*`, Application
		|	Product, `/`, Application;

Sum =		Product
		|	Sum, `+`, Product
		|	Sum, `-`, Product;
=TEX
\end{Fig}

=TEX
In this document, we assume that you are familiar with the basic concepts of context-free grammars.
This example shows the {\SLRP} conventions for writing a grammar: the grammar is
written as a list equations, which we call {\em productions}; the right-hand side of each
production comprises zero or more
{\em alternatives} separated by vertical bars and terminated by a semi-colon;
each alternative comprises zero or more
{\em grammar symbols} separated by commas.
A {\em nonterminal symbol} is a grammar symbol that appears on the left-hand side
of some production.
A {\em terminal symbol} is a grammar symbol that does not appear on the left-hand side
of any production.
A {\em grammar rule} is an equation such as {\em Sum = Product} whose left and
right-hand sides are both drawn from some production in the grammar.

{\SLRP} adopts the convention that the first production
gives the starting symbol for the grammar, in this case {\it Expression}. We call
this symbol the {\em sentence symbol} and we use the term {\em sentence} to refer
to the programs or algebraic expressions or specification
language paragraphs or whatever syntactic objects it may be that the sentence symbol represents.

Nonterminal symbols are written as identifiers, e.g., {\it Sum}, following the Standard ML rules for
forming identifiers (using letters, numbers, underscores and the prime character and
beginning with a letter).
Terminal symbols may be written either as identifiers, e.g., {\it literal}, or as arbitrary strings
enclosed in back-quote characters, e.g.,
=INLINEFT
`+`
=TEX
.

Once you have prepared your grammar file, you can run {\SLRP} to analyse the grammar
and generate the parser code. You use the shell script {\Slrp} to do this specifying the
name of your file using the {\it -f} option. In this case, the name is
{\tt usr032a.grm.txt}. If you want to generate the generic parser, you specify
{\it -g}. While you are developing your grammar, it is generally also useful to specify the
listing option {-l 2}. For our example, the command would be as follows:

=SH
	slrp -g -l 2 -f  usr032a.grm.txt >usr032a.grm.run 2>&1
=TEX
This will result in some messages on standard output (which we have redirected
to the file {\tt usr032a.grm.run}.
It will also produce two output files:

\begin{description}
\item[{\tt usr032a.grm.sml}] containing the Standard ML code
of the generated parser
\item[{\tt usr032a.grm.log}] containing a listing of the grammar and various
additional information, for example, a sorted list of the terminal symbols (which is useful for reference
and to help you spot mistyped nonterminal names).
\end{description}

You can now compile and run the generic parser for your language
after first compiling the parser driver and supporting material using
the ML commands shown in figures~\ref{usr032a.grm.sml1} and~\ref{usr032a.grm.sml2}.
The ML function {\it my\_parse\_file} that this defines takes the name of a file as its argument;
it parses the sequence of terminal symbols in the specified file and prints out the parse tree.
An example input file for it is shown in figure~\ref{usr032a.grm.tst}



\begin{Fig}[ht]{Generic Parser Input}{usr032a.grm.tst}
=DUMP usr032a.grm.tst
	literal  `*`  `(` literal `-` literal
	`)` `+`  identifier `(`
		 literal `/`
		literal 
	`)`
=TEX
\end{Fig}

If you now execute \verb!ae_parser1 "usr032a.grm.tst"!, you will see the print-out of the parse tree shown
in figure~\ref{usr032a.grm.out} (in which the number in brackets after
each terminal symbol is the line number where that instance of the terminal symbol was found).

Of course the functionality of the generic parser as it stands is unlikely to be what you
want in your actual application. Both its ``front-end'', i.e., its lexical analyser, and
its ``back-end'', i.e., the generic reduction actions that build a generic parse tree, will
generally need to be modified to do what is really wanted.
Sections~\ref{COMPLETING} explain how you may go about
mutating the generic parser to meet your needs.


\newpage

\begin{Fig}[th]{An Example Parse Tree}{usr032a.grm.out}
=GFT
1.1.1.1.1.1.1: Atom = literal(1);
1.1.1.1.1.1: Application = Atom;
1.1.1.1.1: Product = Application;
1.1.1.1.3.1.2.1.1.1.1.1: Atom = literal(1);
1.1.1.1.3.1.2.1.1.1.1: Application = Atom;
1.1.1.1.3.1.2.1.1.1: Product = Application;
1.1.1.1.3.1.2.1.1: Sum = Product;
1.1.1.1.3.1.2.1.3.1.1: Atom = literal(1);
1.1.1.1.3.1.2.1.3.1: Application = Atom;
1.1.1.1.3.1.2.1.3: Product = Application;
1.1.1.1.3.1.2.1: Sum = Sum, `-`(1), Product;
1.1.1.1.3.1.2: Expression = Sum;
1.1.1.1.3.1: Atom = `(`(1), Expression, `)`(2);
1.1.1.1.3: Application = Atom;
1.1.1.1: Product = Product, `*`(1), Application;
1.1.1: Sum = Product;
1.1.3.1.3.1.1.1.1.1: Atom = literal(3);
1.1.3.1.3.1.1.1.1: Application = Atom;
1.1.3.1.3.1.1.1: Product = Application;
1.1.3.1.3.1.1.3.1: Atom = literal(4);
1.1.3.1.3.1.1.3: Application = Atom;
1.1.3.1.3.1.1: Product = Product, `/`(3), Application;
1.1.3.1.3.1: Sum = Product;
1.1.3.1.3: Expression = Sum;
1.1.3.1: Application = identifier(2), `(`(2), Expression, `)`(5);
1.1.3: Product = Application;
1.1: Sum = Sum, `+`(2), Product;
1: Expression = Sum;
=TEX
\end{Fig}

=TEX
\newpage
\section{WRITING A COMPLETE PARSER}\label{COMPLETING}
The code generated by {\SLRP} defines a polymorphic function called
=INLINEFT
slrp'gen_parser
=TEX
. This function takes four parameters which identify functions as follows:

\begin{description}
\item[RESOLVER:] a function to resolve ambiguities in the grammar, which we
will refer to as {\em conflicts}, (see section~\ref{CONFLICTS} below for more
information).
If the grammar has no conflicts, the function {\it default\_resolver} provided
as part of the {\SLRP} library will serve for this parameter.
\item[CLASSIFIER] a function to map tokens read from an input stream, e.g.,
``TEMP\_VAR'' or ``+'' to the terminal symbol that represents the lexical
class of the token, e.g., `Identifier' or `+'. A function {\em classifier}
is provided as part of the generic {\SLRP} parser which will serve for
this parameter unless you want to customise the data types used to
represent the lexical classes.
\item[ERROR\_ROUTINE] a function to deal with errors. A function
{\it default\_error} is provided as part of the {\SLRP} library to serve
as an off-the-shelf value for this parameter. See section~\ref{ERRORS}
for more information.
\item[READER] a function to provide input to the parser; this is interface
by which you supply a lexical analyser suiting the rules of the language
you are parsing.
\end{description}

The parser function is also implicitly parametrised by a table generated by {\SLRP}
which maps grammar rules to ML functions.
We can think of the parser {\em reducing} its input to an instance of the sentence symbol by 
matching portions of the input with the right-hand sides of grammar rules and, when
a match is found reducing the matching portion of the input to the name on the left-hand
side of the matching rule.
Each time a rule is used in a reduction the corresponding reduction function is called.
The reduction functions therefore give an operational semantics to the grammar.
In the generic parser we constructed in section~\ref{STARTING}, the code
of the reduction functions was automatically generated by {\SLRP} and the semantics
was calculation of a parse tree. {\SLRP} allows you to write Standard ML code
(called {\em actions}) alongside an alternative in a grammar; when you do this, your
expressions are executed as the reduction functions, giving that alternative the semantics
implied by your design of the actions.

The purpose of this section is to show by example how to write a lexical analyser to
serve as the READER function and how to include applications-specific actions in a grammar.
Our example will be based on the language of arithmetic expressions of figure~\ref{usr032a.grm.txt};
the result will be a calculator program that reads, parses and evaluates the sentences
of the language.
Section~\ref{LEXAN} deals with lexical analysis and section~\ref{ACTIONS} deals with implementing
the actions.

\subsection{Lexical Analyser}\label{LEXAN}
The structure {\it GenericSlrpParser} containing the generic parser support code
provides a framework to help you construct a real lexical analyser.
In this section we show how you can use this framework to implement a lexical analyser
for the language of arithmetic expressions figure~\ref{usr032a.grm.txt}.
The framework helps you with things such as reading an input stream and keeping track of line numbers
leaving you to focus on the lexical matters that are specific to your language.

The lexical analysis framework is presented both as an API and as open source.
If you are writing a parser for production use you will probably eventually want to take a copy
of the source, strip out the parts you do not use, and customise what remains to the requirements
of your application. In this section, we will concentrate on using the API, since that is a the
first step in understanding the design of the framework.

In the lexical analysis framework, a polymorphic type
=INLINEFT
'lc LEX_VALUE
=TEX
\ is used for communication between the lexical analyser and the parser. It is defined as follows:
=GFT
	type 'lc LEX_VALUE = 'lc * (string * int);
=TEX

Here $'lc$ stands for the ML representation of the various terminal symbols (lexical classes)
in the grammar (together with a special end-of-sentence symbol). In the generic parser we
constructed in the previous section $'lc$ is instantiated to a data type
=INLINEFT
LEX_CLASS
=TEX
\ representing the identifiers and string quotations that make up the set of terminal
symbols in the {\SLRP} input format. For our language of arithmetic expressions, we
will continue to use this data type for the lexical classes. It is defined as follows:

=GFT
datatype LEX_CLASS =
		LCIdentifier of string
	|	LCString of string
	|	LCEos;
=TEX

The listing file {\tt usr032a.grm.log} generated by {\SLRP} shows us that we have the following
lexical classes to deal with:
=INLINEFT
identifier,  literal, `(`, `)`, `*`, `+`, `-`, `/`
=TEX
.
Identifiers will be formed from  letters, numbers and underscores and will start with a letter or an underscore.
Literals will be decimal integers.
The punctuation symbols will be as given together with semicolon, the hash symbol
and the equals symbol (for later use)
and our lexical analyser will warn about and skip
over other characters. We will take the input stream to comprise a sequence of
arithmetic expressions separated by semicolons (as might be appropriate in an interactive
program that read and processed arithmetic expressions).
Comments will be allowed and comprise any sequence of characters beginning
with a hash character and terminated by an end-of-line character.
The ML definitions given
in figure~\ref{lex.values} reflect  some of these design decisions.

To construct a lexical analyser using the framework you write what we call recogniser functions for the various
lexical classes.  These are defined in terms of the following data types, a recognisr
function being a function from
=INLINEFT
'lc LEX_STATE
=TEX
\ to itself.

=GFT
datatype CONTINUATION_STATUS =
		InComment
	|	InString of string list;
datatype 'lc LEX_STATUS =
		Unknown
	|	Known of 'lc LEX_VALUE
	|	Comment
	|	Continuation of int * CONTINUATION_STATUS;
type 'lc LEX_STATE = (string list * 'lc LEX_STATUS);
=TEX


The idea is that a recogniser function is passed as the first component
of its argument a buffer comprising a list of strings representing the
unread input characters (one character per string) from the current
input line.  If it recognises what it finds, the function should consume the appropriate
number of characters from the buffer and return the updated buffer together
with a status value indicating what has been found. The generic tools look after
issues such as skipping white space between tokens.

The data types above allow
for lexical values like comments or strings that
are allowed  in some  languages to spread over multiple lines.
We do not need to use these features in our example.

Figures~\ref{lex.punctuation} to~\ref{lex.unknown} show the code for recognising
the lexical classes we are interested in. Because we want to accept a sequence
of arithmetic expressions separated by semicolons, the recogniser for
punctuation maps semicolon to the end of sentence symbol; it also deals
with comments, and, for use when we extend the language a little in section~\ref{ACTIONS}, with
equals signs. The  recognisers for identifiers and numeric literals are straightforwards representations
in ML of finite state machines implementing  our lexical rules. Finally the recogniser for
unknown symbols issues an error message and classifies the erroneous symbol as a comment,
so that the lexical analyser will skip over it.

In figure~\ref{lex.reader}, we use our recogniser functions and the lexical analysis framework API
to construct our READER and then in figure~\ref{lex.parser} we construct the parser
by supplying the appropriate parameters to {\it slrp'gen\_parser} to give a parsing function
that, given a string containing the sentences to be parsed, returns a function which when
called will return the results of parsing the successive sentences in the text.

First of all, we use the function {\it rec\_first} from the API
to combine our individual recognisers. It works by first of all skipping over space characters
and then when a non-space character is found it calls the individual recognisers in turn
until it finds one that signals success (i.e., that does not return status {\it Unknown}).

The READER parameter to {\it slrp'gen\_parser} is defined to have type an instance of
the polymorphic type
=INLINEFT
'st -> 'tok *'st
=TEX
. Here $'st$ is the type of some internal state of the reader and $'tok$ is the type of the
lexical token returned by a call on the READER.
In the lexical analysis framework, $'st$ is instantiated to
=INLINEFT
string list * bool
=TEX
. I.e., the state comprises a buffer of single-character strings representing the as yet
unconsumed contents of an input line together with a flag indicating whether the input
source for the next line is available.

The function {\it gen\_reader}  in the API will automatically produce a READER given
parameters identifying the lexical class representing end-of-input, a recogniser
function and an input source. The input source is represented by a type
{\it IN\_CHAR\_STREAM} defined in the API, which also provides functions
to construct such input streams given a source of input text such as an ML string
or a file name. This provides all that is needed to construct a READER for a parser
that is expecting to find exactly one sentence in the input source. Our example is
a little more complicated, because we want to read a sequence of sentences
separated by semicolons. Our approach in figure~\ref{lex.reader} is to wrap round
a call of the READER function produced by {\it gen\_reader} some code that records the
state of the READER in an assignable variable. 

Given the READER function constructed in figure~\ref{lex.reader}, figure~\ref{lex.parser}
shows the construction of the finished parser. Note that the type {\it IN\_CHAR\_STREAM}
is a record type including a component called {\em close} which we should call to free up
resources, such as open files associated with the input stream. We do not need to close
a stream explicitly if we read to the end of the stream, but we do need to close it explicitly
if the parsing is abandoned before the end of stream is read.

To test the parser, we can execute the following code which will print out the parse trees for
the two expressions ``$1+2$'' and ``$3*4$''.

=GFT
	let	val p = ae_parser2 "1 + 2; 3 * 4";
	in	print_tree(p());
		print_tree(p())
	end;
=TEX

\newpage
\subsection{Semantic Actions}\label{ACTIONS}
In this section, we will complete work on our example by adding
parser action functions to the grammar of the language of arithmetic
expressions, which we now extend to allow the definition of
named numbers. The action functions will actually compute the value of
expression, so that our parser becomes a simple calculator.


The grammar for the extended language is given in figujre~\ref{usr032b.grm.txt}.
The grammar has been extended in two ways: first, the sentence symbol
is now {\it Command}, which is either an expression as before, or a definition
of a named number, comprising an identifier and an expression giving
its value; second, we have added what are called {\it actions}, i.e., Standard ML
expressions in brackets at the end of the alternatives defining an action
to be taken when the alternative is used in parsing a sentence.

\begin{Fig}[ht]{An Action Grammar for Arithmetic Expressions}{usr032b.grm.txt}
=DUMP usr032b.grm.txt
Command =		Expression			(red_command1 x1)
		|	identifier, `=`, Expression	(red_command2 x1 x3);

Expression =	Sum					(red_expression x1);

Atom =		literal				(red_atom1 x1)
		|	identifier			(red_atom2 x1)
		|	`(`, Expression, `)`		(red_atom3 x2);

Application =	Atom				(red_application1 x1)
		|	`-`, Application		(red_application2 x2)
		|	`+`, Application		(red_application3 x2)
		|	identifier, `(`, Expression, `)`
							(red_application4 x1 x3);

Product =		Application			(red_product1 x1)
		|	Product, `*`, Application	(red_product2 x1 x3)
		|	Product, `/`, Application	(red_product3 x1 x3);

Sum =		Product			(red_sum1 x1)
		|	Sum, `+`, Product		(red_sum2 x1 x3)
		|	Sum, `-`, Product		(red_sum3 x1 x3);
=TEX
\end{Fig}
\Hide{
=SH
slrp -g -l 2 -f  usr032b.grm.txt >usr032b.grm.run 2>&1
=TEX
} %\Hide

Each action in a {\SLRP} action grammar is an ML expression which will be evaluated
whenever the parser reduces an instance of the alternative to which the action is
attached. The result of evaluating an action is called the {\em semantic value}
of the instance of the nonterminal symbol on the left-hand side of the grammar rule.
The action is evaluated in the context of a {\tt fn}-expression whose pattern
binds the variables, {\it x1}, {\it x2}, {\it x3}, \ldots to representations of
the semantic values of the 1st, 2nd, 3rd, \ldots grammar symbols in the alternative
(reading it left-to-right). Here, the semantic value of a terminal symbol is
its the lexical value of the input token.
The semantic values are represented in ML as instances of the following polymorphic type:
=GFT
datatype ('tok, 'lc, 'pp) INPUT_STACK_ITEM =
		Token of 'tok * 'lc
	|	Parsed of 'pp;
=TEX

Here $'pp$ (the name stands for partially-parsed) is the type of the semantic value
of a nonterminal symbol and $'tok$ and $'lc$ are the for the lexical value of a token
and its lexical class.  For our calculator, $'pp$ will be integers representing
the results of evaluating arithmetic expressions; $'tok$ and $'lc$
will be the types {\it AE\_LEX\_VALUE} (defined in figure~\ref{lex.values})
and {\it LEX\_CLASS} (discussed in section~\ref{LEXAN}) respectively.
In fact, if you're using the lexical analysis framework,
all you generally need to know about the operands of the constructor {\it Token} is that it has
the form $((\_, (s, \_)), \_)$ where $s$ is the text of the input token.

In our action grammar, we have chose to code a separate ML function for each
alternative in the grammar. We call these the {\em reduction} functions.
This alternative is passed as parameters the $xN$ that
correspond to nonterminals, literals or identifiers in the alternative. So, for
example, the function {\it red\_sum2} is passed the semantic values of the two operands
of the addition.

Figures~\ref{actions.commands} to~\ref{actions.expressions3} give the reduction
functions which give the calculator semantics to our language.
The reduction functions do pattern-matching on their parameters to pick out
the semantic values of the symbols on the right-hand side of the corresponding
grammar rule: a parameter corresponding to a nonterminal has a pattern constructed
with {\it Parsed} and a parameter corresponding to a terminal symbol has a pattern
constructed with {\it Token}.

For example, {\it red\_application4} corresponds to
the rule:
=GFT
Application = identifier, `(`, Expression, `)`
=TEX

Its first parameter is expected to be a token and its second a parsed value.
It implements a semantics supporting two named functions ``abs'' and ``sgn''
which compute absolute value and the sign function. It checks to see that
the name is one of these two and if so returns the value of
the specified function applied to the number appearing in the second parameter.
If the name is not one of the two supported functions it outputs an error
message and raises an exception.

The reduction functions for commands shown in figure~\ref{actions.commands}
also have side effects: they both cause something to be printed on standard output
and {\it red\_command2} also updates a table of named numbers (used in {\it red\_atom2}
to look up the value of an expression comprising an identifier). In general, care should
be taken when reduction functions have side-effects, since the order in which the
reduction functions is called can be tricky to understand. In this case, all we need to
know is that the calling order corresponds to some order for constructing a parse
tree from the bottom up.

Finally, figure~\ref{actions.parser} shows the construction of the finished parser with
semantic actions. It reuses the lexical analyser code described in section~\ref{LEXAN}.
So that it can be tested interactively, we formulate the parser to take its input
from an {\it instream}, such as {\it std\_in}.

If you compile the code in the figures in sections~ref{Compiling}
to~\ref{ActionCode} and run the following ML command, you will
begin an interactive session with our simple calculator program.
=GFT
ae_parser3 std_in;
=TEX

\Hide{
=DUMP usr032b.grm.tst
# One of everything for the calculator program:
1;
ten = 10;
eleven = ten + 1;
twelve = (2  + 2) * 5 - (4 + 4);
one = +1;
neg = -one;
zero = sgn(one) + sgn(neg);
two = abs(one) + abs(one);
three = two + - sgn(neg);
four = twelve/three
=TEX
} %\Hide

If you put a semicolon-separated list of commands in a file, say {\tt usr032b.grm.tst},
then the following command will execute the commands. Note that the calculator
will not allow the file to end with a semicolon. You may find it a useful exercise to try redesigning
the calculator to treat semicolons as terminators rather than separators --- there
are many ways of going about it. One way is discussed in section~\ref{CHECKS} below.
=GFT
ae_parser3 (open_in "usr032b.grm.tst");
=TEX
\newpage

\section{HOW THE PARSERS WORK}\label{HOWITWORKS}
Many useful grammars give rise to problems when you first enter them
into a tool like {\SLRP}; some applications have requirements, e.g., for
error-recovery that are not covered by the simple default error-handling
function we have been using to date. To help you understand the
solutions to some of these problems, this section is intended to
give you a more detailed understanding of how the parsers generated
by {\SLRP} work.

For any context-free grammar, there is a construction, called the LR(0)
construction, which computes from the grammar a rooted directed graph
each of whose edges is labelled either with a grammar symbol
or with a special symbol {\$} meaning end-of-input. An example grammar and
the resulting graph are given in figure~\ref{lr0.automaton}.
The root of the graph is the one drawn in ``inverse video''. The nodes of the graph
are referred to as {\em states}. There is a distinguished state
``accept'', labelled $-1$ in figure~\ref{lr0.automaton}.


\begin{NoBoxFig}[ht]{A Grammar and the Graph of its LR(0) Automaton}{lr0.automaton}
\begin{tabular}{@{}c@{}}
\begin{tabular}{|c|}\hline
\begin{minipage}{0.8\hsize}
=DUMP usr032d.grm.txt
Decl		= UninitDecl, `;`
		|  InitDecl, `;`;

UninitDecl	= var, `:`, type;

InitDecl	= var, `:`, type, `:=`, expr;
=TEX
\end{minipage}\\\hline
\end{tabular}\\\ \\
\epsffile{usr032a.eps}
\end{tabular}
\end{NoBoxFig}


It can be shown that any way of parsing a sentence according to a grammar
can be viewed as an accepting computation of the LR(0) automaton defined
by the graph produced by the LR(0) construction. The LR(0) automaton
is a machine whose input is a string of terminal symbols. The states
of the automaton are the nodes of the graph. The automaton reads its input string symbol
by symbol from left to right, but need not read a new symbol on every transition.
At each stage of its computation it carries out one of the following types of transition,
where $s$ represents its current state:

\begin{description}
\item[Accept:] if it $s$ is ``accept'', then the input
string is a sentence of the language, the sequence of reductions which
have been carried out determine a parse tree for it, and the automaton stops;
\item[Shift:] if there is an edge leading from $s$ to some state $s'$ that is labelled with
the current input symbol, then the automaton can read the next input symbol
and move to state $s'$;
\item[Reduce:] if there is a path in the automaton from some state $s_0$ leading
to $s$ and a grammar rule, $X = \alpha$, such that {\em(i)} the edge labels on the path spell out
$\alpha$ and {\em(ii)} there is an edge labelled with $X$ leading out of $s_0$ to some state
$s'$, then the automaton can move to state $s'$ (without consuming an input symbol);
\item[Error:] if none of the above types of transition is possible, the input string is not
a sentence of the language and the automaton stops.
\end{description}

For example, consider the input ``{\it var, `:', type, `:=', expr}'' for the example  in
figure~\ref{lr0.automaton}. From the start state 10,
the first three input symbols force the automaton to shift three times and enter state 9. 
In this state it has a choice: it can either reduce to state 1 (using the rule
for {\it UninitDecl}), or shift into state 5.
Such a choice is referred to as a LR(0) {\em conflict}.

An LR(1) automaton is like an LR(0) automaton together with a lookahead table telling it
what transition to make in each state for each possible input symbol. If a suitable
table exists with exactly one possible transition for each combination of
state and input symbol, then the grammar is unambiguous and the LR(1) automaton
will provide a deterministic parser for it.
There are several ways of defining such tables. The method used in {\SLRP} is first
to apply the SLR(1) algorithm to compute the LR(0)
graph and then calculate a first approximation to the lookahead table.
This may not eliminate all the conflicts, in which case it applies a rather more
complex method called LALR(1) to attempt to eliminate the remaining conflicts.

The LR(1) automaton is realised in {\SLRP} by the
parser driver code in \cite{DS/FMU/IED/DTD018,DS/FMU/IED/IMP018}.
This implements the LR(1) automaton as an ML function {\it slrp'parse} whcich
takes 8 parameters. The first four of these parameters are values that
are computed by the {\Slrp} program using the above algorithm: the first
gives the initial state of the automaton; the next two (action
table and goto table) together represent the lookahead
table referred to above; the fourth (reduction table) contains the
semantic action functions that are applied whenever a reduction transition is
taken. The function {\em slrp'gen\_parser} is then the result of applying
{\it slrp'parse} to the initial state value and tables that the {\Slrp} program
computes for a particular grammar. The remaining four parameters
of {\it slrp'parse} are then the parameters to {\it slrp'gen\_parser} as discussed
at the beginning of section~\ref{COMPLETING} above.

\section{DEALING WITH PARSING CONFLICTS}\label{CONFLICTS}

It is a feature of the way the LR(0) graph is constructed that in any given state there
is at most one way of shifting.
There are two kinds of conflicts, shift/reduce conflicts and reduce/reduce conflicts

\begin{Fig}[ht]{An Inherently Ambiguous Grammar}{usr032e.grm.txt}
=DUMP usr032e.grm.txt
	S	= AB, C | A, BC;
	AB	= | a, AB, b;
	C	= | C, c;
	A	= | A, a;
	BC	= b, BC, c;
=TEX
\end{Fig}

\begin{Fig}[ht]{Resolving Ambiguities By Widening the Language}{usr032f.grm.txt}
=DUMP usr032f.grm.txt
	S	= A, B, C	
			((fn Parsed  m => fn Parsed n => fn Parsed p =>
			if	m <> n andalso n <> p
			then	raise SYNTAX_ERROR
			else	0) x1 x2 x3);
	A	= 		( 0 )
		|   A, a		((fn Parsed m => m + 1) x1);
	B	=		( 0 )
		|   B, b		((fn Parsed n => n + 1) x1);
	C	=		( 0 )
		|   C, c		((fn Parsed p => p + 1) x1);
=TEX
\end{Fig}

\begin{Fig}[p]{A non-LALR(1) Grammar}{usr032g.grm.txt}
=DUMP usr032g.grm.txt
Command =		Expression
		|	identifier, `(`, FormalParams, `)`, `=`,  Expression;

Expression = Sum;

FormalParams =	identifier
		|	FormalParams, `,`, identifier;

Atom =		literal
		|	identifier
		|	`(`, Expression, `)`;

Application =	Atom
		|	`-`, Application
		|	`+`, Application
		|	identifier, `(`, ExpressionList, `)`;

ExpressionList =	Expression
		|	ExpressionList, `,`,  Expression;

Product =		Application
		|	Product, `*`, Application
		|	Product, `/`, Application;

Sum =		Product
		|	Sum, `+`, Product
		|	Sum, `-`, Product;
=TEX
\end{Fig}
=SH
sed -e '/^ExpressionList/,/;/d' \
	-e '/^Application/,/;/d' \
	< usr032g.grm.txt > usr032h.grm.txt
=TEX
\begin{Fig}[p]{Making a Grammar LALR(1)}{usr032h.grm.txt}
=DUMPMORE usr032h.grm.txt
Application =	Atom
		|	`-`, Application
		|	`+`, Application
		|	identifier, `(`, NonIdExpressionList, `)`
		|	identifier, `(`, FormalParams, `)`;

NonIdExpressionList =	NonIdExpression
		|	FormalParams, `,`, NonIdExpression
		|	NonIdExpressionList, `,`, Expression;

NonIdExpression =		`(`, Expression, `)`
		|	`-`, Application
		|	`+`, Application
		|	identifier, `(`, NonIdExpressionList, `)`
		|	identifier, `(`, FormalParams, `)`
		|	Product, `*`, Application
		|	Product, `/`, Application
		|	Sum, `+`, Product
		|	Sum, `-`, Product;
=TEX
\end{Fig}


\newpage
\section{OTHER CHECKS THAT {\SLRP} MAKES}\label{CHECKS}
\begin{Fig}[p]{A Grammar for a Non-terminating Parser}{usr032i.grm.txt}
=DUMP usr032i.grm.txt
CommandStream =		GotCommand, `;`, CommandStream	(0);

GotCommand =		Command		(red_got_command x1);
=TEX
\end{Fig}
=SH
cat usr032b.grm.txt >>usr032i.grm.txt
=TEX
=IGN
ae_parser4 std_in;
1+2;
x = 99;
y = x;
ae_parser4 (open_in "usr032b.grm.tst");
=TEX
\section{ERROR HANDLING}\label{ERRORS}

TBS

\clearpage
\appendix
\section{STANDARD ML CODE EXAMPLES}
\subsection{Compiling {\SLRP} Parsers}\label{Compiling}
The Standard ML code in the figures in this section compiles all the library support needed for the generic parser for
arithmetic expressions described in section~\ref{STARTING}
and then compiles and instantiates the code generated by {\SLRP}. If you are
working in a {\Product} ML database, then you can omit the first six files since
they are already compiled into {\Product}.

\begin{Fig}[h]{Compiling the {\SLRP} Library Code}{usr032a.grm.sml1}
=SML
map use [
	"dtd108.sml",	(* Portability infrastructure *)
	"imp108.sml",
	"dtd002.sml",	(* System control and error reporting *)
	"imp002.sml",
	"dtd001.sml",	(* Standard ML utilities *)
	"imp001.sml",
	"dtd018.sml",	(* SLRP parser driver *)
	"imp018.sml",
	"dtd118.sml",	(* Generic SLRP parser support *)
	"imp118.sml"
	];
open GenericSlrpParser;
=TEX
\end{Fig}

\begin{Fig}{Compiling the Code Generated by {\SLRP}}{usr032a.grm.sml2}
=SML
use"usr032a.grm.sml";		(* The generated parser code *)
val ae_parser1 : string -> unit =
	print_tree o parse_file slrp'gen_parser;
=TEX
\end{Fig}
=TEX

\Hide{
=SML
 ae_parser1 "usr032a.grm.tst";
=TEX
}

\clearpage
\subsection{A Lexical Analyser}\label{LexAnCode}

The Standard ML code in the figures in this section implements the lexical analyser
for the language of arithmetic expressions as discussed in section~\ref{LEXAN} above.

\begin{Fig}[h]{Constructing Lexical Values}{lex.values}
=SML
type AE_LEX_VALUE = LEX_CLASS LEX_VALUE;
type AE_LEX_STATE = LEX_CLASS LEX_STATE;
fun lv_identifier (s : string) : AE_LEX_VALUE = (
	(LCIdentifier "identifier", (s, get_line_number()))
);
fun lv_literal (s : string) : AE_LEX_VALUE = (
	(LCIdentifier "literal", (s, get_line_number()))
);
fun lv_punctuation c = (LCString c, (c, get_line_number()));
val lv_left_bracket : AE_LEX_VALUE = lv_punctuation "(";
val lv_right_bracket : LEX_CLASS LEX_VALUE = lv_punctuation ")";
val lv_times : AE_LEX_VALUE = lv_punctuation "*";
val lv_plus : AE_LEX_VALUE = lv_punctuation "+";
val lv_minus : AE_LEX_VALUE = lv_punctuation "-";
val lv_over : AE_LEX_VALUE = lv_punctuation "/";
val lv_equals : AE_LEX_VALUE = lv_punctuation "=";
val lv_semicolon : AE_LEX_VALUE = (LCEos, (";", get_line_number()));
val lv_end_of_input : AE_LEX_VALUE =
	(LCEos, ("<end-of-input>", get_line_number()));
=TEX
\end{Fig}


\begin{Fig}{Recognising Punctuation Symbols}{lex.punctuation}
=SML
fun rec_punctuation (("(" :: more, _)  : AE_LEX_STATE)
	: AE_LEX_STATE
	= (more, Known  lv_left_bracket)
|   rec_punctuation ( ")" :: more, _ ) = (more, Known lv_right_bracket)
|   rec_punctuation ( "*" :: more, _ ) = (more, Known lv_times)
|   rec_punctuation ( "+" :: more, _ ) = (more, Known lv_plus)
|   rec_punctuation ( "-" :: more, _ ) = (more, Known lv_minus)
|   rec_punctuation ( "/" :: more, _ ) = (more, Known lv_over)
|   rec_punctuation ( ";" :: more, _ ) = (more, Known lv_semicolon)
|   rec_punctuation ( "=" :: more, _ ) = (more, Known lv_equals)
|   rec_punctuation ( "#" :: _, _ ) = ([], Comment)
|   rec_punctuation (chs, _) = (chs, Unknown);
=TEX
\end{Fig}


\begin{Fig}{Recognising an Identifier}{lex.identifier}
=SML
fun is_alph_or_us ch = (
		"a" <= ch andalso ch <= "z"
	orelse	"A" <= ch andalso ch <= "Z"
	orelse	ch = "_"
);
fun is_digit ch = "0" <= ch andalso ch <= "9";
val is_alnum = is_alph_or_us fun_or is_digit;
fun rec_identifier (([], _) : AE_LEX_STATE) : AE_LEX_STATE = (
	([], Unknown)
) | rec_identifier (chs as (ch :: more), _) = (
	let	fun aux acc [] = (implode (rev acc), [])
		|   aux acc (cs as (c::more)) = (
			if	is_alnum c
			then	aux (c::acc) more
			else	(implode(rev acc), cs)
		);
	in	if	is_alph_or_us ch
		then	let	val (name, rest) = aux [ch] more;
			in	(rest, Known (lv_identifier name))
			end
		else	(chs, Unknown)
	end
);
=TEX
\end{Fig}

\begin{Fig}{Recognising a Literal}{lex.literal}
=SML
fun rec_literal (([], _) : AE_LEX_STATE)
	: AE_LEX_STATE = ([], Unknown)
|   rec_literal (chs as (ch :: more), _) = (
	let	fun aux acc [] = (implode (rev acc), [])
		|   aux acc (cs as (c::more)) = (
			if	is_digit c
			then	aux (c::acc) more
			else	(implode(rev acc), cs)
		);
	in	if	is_digit ch
		then	let	val (digits, rest) = aux [ch] more;
			in	(rest, Known (lv_literal digits))
			end
		else	(chs, Unknown)
	end
);
=TEX
\end{Fig}

\begin{Fig}{Dealing with Lexical Errors}{lex.unknown}
=SML
fun rec_unknown (( ch :: more, _ ) : AE_LEX_STATE)
	: AE_LEX_STATE = (
	output(std_out, "Unrecognised input character \"" ^ ch ^ "\"\n");
	(more, Comment)
) | rec_unknown ([], _) = ([], Unknown);
=TEX
\end{Fig}

\begin{Fig}{Constructing the Reader}{lex.reader}
=SML
val rec_token : AE_LEX_STATE -> AE_LEX_STATE =
	rec_first[rec_punctuation, rec_identifier, rec_literal, rec_unknown];
val reader_state : (string list * bool) ref = ref ([], true);
fun reader (strm : IN_CHAR_STREAM) 
	: (AE_LEX_VALUE, string list * bool) READER = (
	let	val do_read = gen_reader LCEos rec_token strm
	in	fn state =>
		let	val (tok, state') = do_read state;
		in	reader_state := state';
			(tok, state')
		end
	end
);
=TEX
\end{Fig}

\begin{Fig}{Constructing the Parser}{lex.parser}
=SML
use"usr032a.grm.sml";
fun ae_parser2
	(text : string)  : unit -> LEX_CLASS GEN_PARSE_TREE = (
	let	val strm as {close, ...} = in_char_stream_of_string text;
		val do_parse =
			slrp'gen_parser
			default_resolver
			classifier
			(default_error string_of_lex_value)
			(reader strm);
		val _ = reader_state := ([], true);
	in	fn () => do_parse (!reader_state)
		handle ex => (
			close();
			raise ex
		)
	end	
);
=TEX
\end{Fig}

\clearpage
\subsection{Adding Actions}\label{ActionCode}

The Standard ML code in this section implements the reduction functions
that support the action grammar shown in figure~\ref{usr032b.grm.txt} in
section~\ref{ACTIONS} above.

\begin{Fig}[h]{The Reduction Functions for Commands}{actions.commands}
=SML
val named_numbers : int S_DICT ref = ref [];
fun red_command1 (Parsed i) = (
	named_numbers := s_enter "it" i (!named_numbers);
	output(std_out, "it = " ^ string_of_int i ^ "\n");
	i
);
fun red_command2 (Token ((_, (s, _)), _)) (Parsed i) = (
	named_numbers := s_enter s i (!named_numbers);
	output(std_out, s ^ " = " ^ string_of_int i ^ "\n");
	i
);
=TEX
\end{Fig}


\begin{Fig}{The Reduction Functions for Expressions I}{actions.expressions1}
=SML
fun red_accept (Parsed i) = i;
val red_expression = red_accept;
fun red_atom1 (Token ((_, (s, _)), _)) = nat_of_string s; 
fun red_atom2 (Token ((_, (s, _)), _)) = (
	case s_lookup s (!named_numbers) of
		Value i => i
	|	Nil => (
		output(std_out, "Undefined name \"" ^ s ^ "\"\n");
		raise SYNTAX_ERROR
	)
);		
=TEX
\end{Fig}


\begin{Fig}{The Reduction Functions for Expressions II}{actions.expressions2}
=SML
val red_atom3 = red_accept;
val red_application1 = red_accept;
fun red_application2 (Parsed i) =  ~i;
val red_application3 = red_accept;
fun red_application4 (Token((_, (s, _)), _)) (Parsed i) = (
	case s of
		"abs" => if i < 0 then ~i else i
	|	"sgn" => if i > 0 then 1 else if i = 0 then 0 else ~1
	|	_ => (
		output(std_out, "Unsupported function \"" ^ s ^ "\"\n");
		raise SYNTAX_ERROR
	)
);
=TEX
\end{Fig}

\begin{Fig}{The Reduction Functions for Expressions III }{actions.expressions3}
=SML
val red_product1 = red_accept;
fun red_product2 (Parsed i) (Parsed j) = i * j;
fun red_product3 (Parsed i) (Parsed j) = i div j handle Div => (
	output(std_out, "Zero divisor\n");
	raise SYNTAX_ERROR
);
val red_sum1 = red_accept;
fun red_sum2 (Parsed i) (Parsed j) = i + j;
fun red_sum3 (Parsed i) (Parsed j) = i - j;
=TEX
\end{Fig}

\begin{Fig}{Constructing the Parser}{actions.parser}
=SML
use"usr032b.grm.sml";
fun ae_parser3
	(instrm : instream)  : unit = (
	let	val strm as {close, ...} = in_char_stream_of_instream instrm;
		val do_parse =
			slrp'gen_parser
			default_resolver
			classifier
			(default_error string_of_lex_value)
			(reader strm);
		val _ = reader_state := ([], true);
	in	while case !reader_state of ([], false) => false | _ => true do
			(do_parse (!reader_state); ())
			handle SYNTAX_ERROR =>  (
				(if	not (ExtendedIO.is_term_in instrm)
				then	close()
				else	());
				raise SYNTAX_ERROR
			)
	end
);
=TEX
\end{Fig}


\clearpage
\subsection{Non-terminating Parser}
The Standard ML code in this section implements the non-terminating parser
corresponding to the grammar given in figure~\ref{usr032i.grm.txt} in section~\ref{CHECKS}.

\begin{Fig}[h]{Recognising Punctuation Symbols for the Stream Parser}{nonterminating.punctuation}
=SML
val lv_semicolon : AE_LEX_VALUE = lv_punctuation ";";
fun rec_punctuation2 (("(" :: more, _)  : AE_LEX_STATE)
	: AE_LEX_STATE
	= (more, Known  lv_left_bracket)
|   rec_punctuation2 ( ")" :: more, _ ) =
	(more, Known lv_right_bracket)
|   rec_punctuation2 ( "*" :: more, _ ) =
	(more, Known lv_times)
|   rec_punctuation2 ( "+" :: more, _ ) =
	(more, Known lv_plus)
|   rec_punctuation2 ( "-" :: more, _ ) =
	(more, Known lv_minus)
|   rec_punctuation2 ( "/" :: more, _ ) =
	(more, Known lv_over)
|   rec_punctuation2 ( "=" :: more, _ ) = 
	(more, Known lv_equals)
|   rec_punctuation2 ( ";" :: more, _ ) = 
	(more, Known lv_semicolon)
|   rec_punctuation2 ( "#" :: _, _ ) = 
	([], Comment)
|   rec_punctuation2 (chs, _) = (chs, Unknown);
=TEX
\end{Fig}

\begin{Fig}{Constructing the Non-terminating Parser}{nonterminating.parser}
=SML
val rec_token : AE_LEX_STATE -> AE_LEX_STATE =
	rec_first[rec_punctuation2, rec_identifier, rec_literal, rec_unknown];
fun reader2 (strm : IN_CHAR_STREAM) 
	: (AE_LEX_VALUE, string list * bool) READER = (
	let	val do_read = gen_reader LCEos rec_token strm
	in	fn state =>
		let	val (tok, state') = do_read state;
		in	reader_state := state';
			(tok, state')
		end
	end
);
fun red_got_command (Parsed i) = (
	output(std_out, string_of_int i ^ "\n");
	i
);
use"usr032i.grm.sml";
fun ae_parser4
	(s : instream)  : unit = (
	let	val do_parse =
			slrp'gen_parser
			default_resolver
			classifier
			(default_error string_of_lex_value)
			(reader2(in_char_stream_of_instream s));
	in	(do_parse ([], true); ()) handle SYNTAX_ERROR => ()
	end
);
=TEX
\end{Fig}

\clearpage
\section{COMMAND LINE INTERFACE}\label{COMMAND}
TBS

\newpage
\section{SLRP INPUT FORMAT}\label{INPUTFORMAT}
TBS

\newpage
\section{STANDARD ML  LIBRARY}\label{LIBRARY}
TBS

\newpage
\section{PARSER DRIVER INTERFACE}\label{DRIVERINTERFACE}
TBS
\Hide{
=SH
slrp -g -l  4 -f  usr032d.grm.txt >usr032d.grm.run 2>&1
for c in e f g h i
do
	slrp -g -l 2 -f  usr032$c.grm.txt >usr032$c.grm.run 2>&1
done
=TEX
} %\Hide

\end{document}

