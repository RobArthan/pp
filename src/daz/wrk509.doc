% %Z% $Date$ $Revision$ $RCSfile$
=TEX
% TQtemplate.tex
% use_file "wrk509";
% doctex wrk509x[0-9][0-9].th
% docdvi wrk509
\documentstyle[hol1,11pt,TQ,hyperbasics]{article}
\ftlinepenalty=9999
\def\Hide#1{}
\def\Bool{``$\it{:}bool\,$''}
\makeindex
\TPPproject{DAZ PROJECT}  %% Mandatory field
%\TPPvolume{}
%\TPPpart{}
\TPPtitle{Analysis of Declarations}  %% Mandatory field
\TPPref{ISS/HAT/DAZ/WRK509}  %% Mandatory field
\def\SCCSversion{$Revision$%
}
\TPPissue{\SCCSversion}  %% Mandatory field
\TPPdate{\FormatDate{$Date$%
}}  %% Mandatory field (with sensible default)
\TPPstatus{Draft}
%\TPPstatus{Informal}
\TPPtype{Technical}
%\TPPkeywords{HOL}
\TPPauthor{K.~Blackburn & WIN01}  %% Mandatory field
%\TPPauthors{Name 1&location 1\\Name 2&location 2\\Name 3&location 3}
\TPPauthorisation{R.D.~Arthan & HAT Team}
\TPPabstract{%
As part of the requirements analysis for proof tools for the Compliance Notation, the generation of Z paragraphs has been analysed for basic declarations.
This paper reports on that analysis.}
%\TPPabstractB{}
%\TPPabstractC{}
%\TPPabstractD{}
%\TPPabstractE{}
%\TPPabstractF{}
\TPPdistribution{\parbox[t]{4.0in}{%
      Library}}

%\TPPclass{CLASSIFICATION}
\newfont{\icllogo}{icllogo50}
\def\TPPheadlhs{$\vcenter{\halign{##\cr\icllogo ICL\cr}}$}
%\def\TPPheadlhs{}
%\def\TPPheadcentre{}
%def\TPPheadrhs{}
%\def\TPPfootlhs{}
%\def\TPPfootcentre{}
%\def\TPPfootrhs{}

\begin{document}
\TPPsetsizes
\makeTPPfrontpage

\vfill
\begin{centering}

\bf Copyright \copyright\ : International Computers Ltd \number\year

\end{centering}

\newpage
\section{DOCUMENT CONTROL}
\subsection{Contents List}
\tableofcontents
%\subsection{Document Cross References}
\bibliographystyle{fmu}
\bibliography{fmu,daz}
\subsection{Changes History}  % to get section number `0.3'
\begin{description}

\item[Issue 1.1-1.2] Initial Drafts.
\item[Issue 1.3]
Added thoughts on possible usage.
\item[Issue 1.4]
Extensive changes after RDA comments.
\end{description}
\subsection{Changes Forecast}
As called for by peer review.
\pagebreak
\section{GENERAL}
\subsection{Scope}
This document reports on a study called for in \cite{ISS/HAT/DAZ/PLN010,ISS/HAT/DAZ/PLN012}.
\subsection{Introduction}
As part of the requirements analysis for proof tools for the Compliance Notation, the generation of Z paragraphs has been analysed for basic declarations.
This paper reports on that analysis.

The approach we take is to consider the different categories of basic declaration syntax
on a case by case basis, and then draw together these results.
This follows the categories given in \cite{DRA/CIS/CSE3/TR/94/27/2.1}.

Thus this document is organised as follows: 
\begin{itemize}
\item
in section \ref{prelim} we consider some issues that pervade the following analysis;
\item
in section \ref{categ} we analyse each of the forms of basic declaration by category;
\item
in section \ref{summary} we summarise the findings of the previous section, and consider the possible contexts when using the analysis;
\item
and in \ref{recomm} we recommend what approaches should be taken to provide proof support
for this area of Compliance Tools Notation processing.
\end{itemize}

\section{PRELIMINARY NOTES}\label{prelim}

\subsection{Pseudo-Z}
In the following discussions we use $name$ to indicate the name of a Z constant, such
as a type name or an enumerated item, subscripting $name$ if necessary to remove ambiguity.
We use $integer\_literal$ to indicate an integer literal, perhaps generated from
static evaluation of an expression.
We use the Compliance Notation $\_\_$ connective to indicate the attributes of a type name,
by connecting $name$ to the attribute suffix.

The results of this are sometimes not Z, but are meant to highlight the structure of theorems, etc., for the purpose of discussing the usefulness of the items for rewriting.

\subsection{A Hierarchy of Usefulness}
The definitions and axioms generated during the processing of Compliance Tool Notation 
will presumably all play a part in a proof concerning the items defined or axiomatised.
However, it would be very slow, and potentially likely to ``explode'' goals into being unmanagably large if we always rewrote with all of these theorems.
What would be useful, therefore, is a way of categorising theorems as to when we should use them
for rewriting.

Having seen the need for such categorisation, we can generalise the concept, to consider not just rewriting with theorems, but the application of any conversion to a term (where
the application of a rewriting conversion using a theorem is just one possibility).

One can do the categorisation based on a specific application. 
For instance, if one was concerned with determining the Z type of function applications, then
only rewrites of interest would be ones dealing with function arrows or function applications.
Alternatively, categorisation can be based on the complexity of the of the result of the conversion (or of the rewriting), and this could be done as follows:

A conversion can be categorised as to its likely usefulness by its complexity as follows.
\begin{description}
\item[Class 1]
Appropriate to be a conversion for the default rewrite rules, etc., in a straightforward proof context in the appropriate theory.

Such a conversion should simplify the term it is applied to, and not lengthen it (by count of tokens). 
Appropriate conversions for this class would for example be ones that replace constants by literals.

It is still possible that a user will not want the conversion applied in all cases, but only with the sort of frequency that, for instance pure rewriting is chosen over rewriting with the current proof context rewrite rules.

\item[Class 2]
Appropriate to be a conversion used within a straightforward automatic proof procedure (e.g. the one for $prove\_tac$ in a straightforward proof context), but not one that should ``always'' be applied.
A user would probably want easy access to such a conversion.

Such a conversion should simplify (in some sense) the term it is applied to, but may lengthen it to some extent.
Appropriate conversions would be ones that move the proof into a ``lower'' theory, 
such as rewriting a Compliance Notation expression into a term only using operators from the theory of relations, when reasoning about the Compliance Notation form of the term is unlikely to progress any other way.

\item[Class 3]
Appropriate only to automatic proof tools that take a ``brutal'' approach to
a proof, for instance,  conversion that simplifies, but greatly lengthens, a term, whose output is not intended to be human readable.
Such a conversion will not often be of use in a manual proof.

Some of the extensionality conversions would fit in this class.

\item[Class 4]
A conversion that is has no appropriate use except for careful manual use.

This includes many recursive definitions and conversions that reverse the effect of 
conversions from other classes.
\end{description}

\subsection{Static Expressions}
Various expression slots in declarations must be given static expressions in SPARK. 
It should be noted that this does not necessarily mean that they can be evaluated 
within \ProductZ.
This is for a variety of reasons, such as an inability to reason about
real numbers in \ProductZ\ to the same extent as SPARK.
Such reasons normally manifest by some constituent of the formal expansion of such a 
value being axiomatically introduced without an actual definition (i.e. as how a real number would be handled).
However, any expression that is meant to be statically evaluated to a $˙$ value
is assumed to be a good candidate for a ``once and for all'' attempt at
evaluation in \ProductZ, rather than being left for repeated attempts during
individual proofs.

The may also be problems handling predecessor and successor functions in static evaluations
carried out under \ProductZ.

\subsection{Other Z Paragraph Generating Parts of the Compliance Notation}
Some of the conclusions reached below, from considering all the possibilities for 
Basic Declarations, can be applied to the other areas of the Compliance Notation that 
cause Z paragraphs to be generated (we are not interested in Z embedded in the Notation
by the user).
The only significant instance of generating paragraphs is the case of subprogram declarations.

\subsection{Ranges Versus Set Displays}
The reasoning we are proposing takes place in a context where we have access to some
moderately powerful linear arithmetic tools.
In particular these tools are capable of reasoning about the inequalities that can be
derived from a range expression, but might well have difficulties using the results of
expanding ranges into set displays (which will give predicates of the form $x = n ≤ x = n+1 ≤ ...$).
This suggests that, in our area of particular interest, range expressions are preferable even to a two element set display of numbers.

\subsection{Type Marks}
Note that the specification of SPARK uses ``type mark'' for ``type name or subtype name'',
and this wording is used in this document.
In SPARK all syntax (other than type definitions) that requires a type demands a type mark,
i.e. a name that will be associated with a definition.
That is, SPARK does not allow anonymous types. 
This simplifies the variety of possible syntax to be considered.

\section{CATEGORIES OF BASIC DECLARATIONS}\label{categ}

\subsection{Variables}
Variable declarations do not directly cause the introduction of Z
paragraphs, and are therefore not further considered in this document.

\subsection{Constants}
πCN
name : constant name_type := exp;
∞
The SPARK specification and the actual \ProductZ\ implementation allows multiple constant identifiers to be 
introduced in one statement, but this is not covered in the Compliance Notation
specifications (they are implicitly handled as multiple individual declarations).
$exp$ must be a static expression in SPARK.

A declaration of a constant will either introduce a global definition or an axiom, depending on whether the constant's expression can be translated into Z.

These Z paragraphs are either of the form:
πZ
name == exp
∞
or:
πZ
name : name_type
∞

A theorem providing the rewrite of the name to the best available evaluation of the expression in the first case 
would of use (for it must be a static expression, so evaluation within \ProductZ\ is at least possible).
Such a theorem would be in Class 1 if the expression can be evaluated to a literal or other previously defined constant, and otherwise would probably fall in class 2 (and the worst cases in Class 3), and would make the original
global definition of no further use.

There is no obvious processing for the second case, but the axiom (used as a membership statement of the form $name ç type § true$, perhaps normalised with other Class 1  conversions) is in Class 1.

\subsection{Enumeration Types}\label{enumerate}
πCN
type name is (nameâ0, ..., nameân)
∞
There must be at least 2 enumeration literals.

The introduction of an enumeration type introduces generic definitions of the enumeration literals, the type itself, and the type's attributes.
In more detail:

There will be one global definition per enumeration literal, of the form:
πZ
name == integer_literal
∞
Such theorems are in class 1.

The global definition of the type is of the form:
πZ
name == nameâ0 .. nameân
∞
This can be rewritten to:
πZ
Ù name = 0 .. integer_literal
∞
or various other obvious forms, e.g. involving inequalities, or simplifications
if there are few enumerated elements.
The integer literal will be one less than the number of enumerated items.
This rewritten theorem belongs in Class 2, and would make the original
global definition of no further use.

The attributes are as follows:

πZ
name__FIRST == nameâ0
name__LAST == nameân
∞
which can be rewritten to:
πZ
Ù name__FIRST = 0
Ù name__LAST = integer_literal
∞
These rewritten theorems belong in Class 1, and would make the original
global definitions of no further use.
The integer literal will be one less than the number of enumerated items.

πZ
name__SUCC == (name \ {name__LAST}) Ú succ
∞
This has a variety of possible rewrites. The two extremes might be:
πZ
Ù name__SUCC == (0 .. integer_literal) Ú succ
Ù name__SUCC == {(0,1), ..., (integer_literalân, integer_literalËn+1Í)}
∞
In general, the first belongs in Class 2, the second in Class 3. 
However, if the enumerated type is small (2, maybe 3, elements), then 
then simplifications can be made to the first rewritten theorem, and the results moved up the classes.
Any of these would make the original
global definition of no further use.
Some rewriting use may be made of the fact that $succ$ is already domain restricted to $Ó$.

We perhaps more usefully could introduce the theorem:
πZ
Ù µ i : 0 .. integer_literal ∑ name__SUCC i = i + 1
∞
where $integer_literal$ was the number of enumerated items minus 2.

πZ
name__PRED == ((name \ {name__LAST}) Ú succ)~
∞
This has a variety of possible rewrites. The two extremes might be:
πZ
Ù name__PRED == (1 .. integer_literal) Ú (succ ~)
Ù name__PRED == {(1,0), ..., (integer_literalân, integer_literalËn-1Í)}
∞
In general, the first belongs in Class 2, the second in Class 3. 
However, if the enumerated type is small (2, maybe 3, elements), then 
then simplifications can be made to the first rewritten theorem, and both the rewritten theorem and the second global definition moved up the classes.
Any of these would make the original
global definition of no further use.
Some rewriting use may be made of the fact that the range of $succ$ is restricted to $Óâ1$.

We perhaps more usefully could introduce the theorem:
πZ
Ù µ i : 1 .. integer_literal ∑ name__PRED i = i - 1
∞
where $integer_literal$ was the number of enumerated items minus 1.

πZ
name__POS == id name
name__VAL == name__POS~
∞
The second can be rewritten to:
πZ
Ù name__VAL = name__POS
∞
The rewritten theorem should probably be in Class 1, the first global definition should be in Class 2 (it is too expansive, given the follow-on rewrite of $name$, to be in Class 1), and would make the original
global definition of no further use.
A rewrite theorem of the following form might be put in Class 3 if the type is small (if very small, perhaps even in Class 2):
πZ
Ù name__POS = {(0,0),...(integer_literalân, integer_literalân)}
∞

We perhaps more usefully could introduce the theorems:
πZ
Ù µ i : 0 .. integer_literal ∑ name__POS i = i
Ù µ i : 0 .. integer_literal ∑ name__VAL i = i
∞
where $integer_literal$ was the number of enumerated items minus 1.

\subsection{Array Types}
Constrained array type declarations are of the form:
πCN
type name is array (indexâ1, ..., indexân) of comp;
∞
The indexes must be type marks for discrete, and thus static, ranges. 

The introduction of a constrained array type with component type mark $comp$ will introduce global definitions for the type itself and, if only 1-dimensional, its attributes.

πZ
name == indexâ1 ∏ ... ∏ indexân ≠ comp
∞
This definition is in Class 2 or maybe even 3, depending on the number and character of the index types.

For 1-dimensional arrays the attribute definitions are:
πZ
name__FIRST == indexâ1__FIRST
name__LAST == indexâ1__LAST
name__LENGTH == # indexâ1
name__RANGE == indexâ1
∞
All but the 3rd of these are in Class 1, the 3rd one being in Class 2.
It is possible that $\# indexâ1$ can be rewritten to a simpler value, maybe even an
integer literal (for instance $\# (n .. m)$, assuming a likely form of the underlying index type, could be rewritten to $m + 1 - n$). 
This may move the rewritten theorem to Class 1, and would make the original
global definition of no further use.

Unconstrained array type declarations are of the form:
πCN
type name is array (indexâ1 range <>, ..., indexân range <>) of comp;
∞
The indexes are arbitrary type marks, apparently, and in particular do not
appear to need to be static in SPARK.

The introduction of an unconstrained array type only introduces an axiom of the form:
πZ
name :  (indexâ1 ∏ ... ∏ indexân ﬂ comp)
∞
This axiom, used as a membership statement, perhaps normalised by applying other applicable Class 1 conversions, is probably in Class 1, and could also be used in 
resolution proof attempts.

\subsection{Record Types}
πCN
type name is 
record
	labâ1 : ltypeâ1;
	...
	labân : ltypeân;
end record;
∞
The Compliance Notation seems to address the issue of multiple
identifiers in a single component declaration by treating them as multiple declarations(?).
The $ltype$s are all type marks.

The introduction of a record type will introduce the following style of schema:
ˇ name ¸¸¸¸¸¸¸¸¸¸¸
‹ labâ1 : ltypeâ1
‹ ...
‹ labân : ltypeân
à¸¸¸¸¸¸¸¸¸¸¸¸¸¸
but as a horizontal schema, i.e.
πZ
name == [labâ1 : ltype; ...; labân : ltypeân]
∞
This equation of itself is probably in Class 2 or 3.
It is also something that perhaps ought to be rewritten with in some circumstances, 
regardless of the size of the horizontal schema, as it is crucial for progress.
It can also be used to generate various type membership theorems for the projections, that might be
useful (and thus perhaps even in Class 1 due to their simple nature).

\subsection{Integer Types}
πCN
type name is expâ1 .. expâ2
∞
(range attributes are valid SPARC range constraints, but are not handled formally)

Integer type declarations are types with the range constraint being statically determinable. 
Their introduction causes the introduction of a global definition of themselves and either global definitions or axioms for their attributes.

πZ
name == expâ1 .. expâ2
∞
This is a Class 2 theorem, and there may be possible rewrites of it, as
the two expressions should be static.

πZ
name__FIRST == expâ1
name__LAST == expâ2
∞
Rewritten theorems based on the evaluation of the expressions should be in Class 1 if the results are integer literals or other constants, and Class 2 otherwise, and would make the original
global definitions of no further use.

πZ
name__SUCC : ˙ ﬂ ˙
name__PRED : ˙ ﬂ ˙
name__POS : ˙ ﬂ ˙
name__VAL : ˙ ﬂ ˙
∞
These, taken as membership statements are in Class 1.
Sadly, the translation of the Compliance Notation does not give the obvious definitions for
these constants, only axioms.
This is to do with worries about overflows.

\subsection{Real Types}
πCN
type name is delta ...
type name is digits ...
∞

The introduction of a real type introduces the type as a given set:
πZ
[ name ]
∞
This may be used to rewrite any predicate of the form $r ç name$ to $true$
(i.e. U-elimination). 

\subsection{Subtypes}
πCN
subtype name is master constraint
∞
The constraint can be range, floating point, fixed point or index, and, from SPARK, must
be statically determinable.

There are various possibilities for subtypes, being whether or not they are subtypes of types that are translated into $˙$, and on the nature of their constraint (e.g. can it be translated into a range constraint).

\subsubsection{Case 1}
If the master type, $master$, can be translated to $˙$ and the constraint can be translated to a $˙$ range constraint $expâ1 .. expâ2$ global definitions are introduced for the type and its attributes:
πZ
name == expâ1 .. expâ2
∞
The expressions must be static, and thus we can hope to be able to 
rewrite this to something of the form:
πZ
Ù name = integer_literalâ1 .. integer_literalâ2
∞
Such a theorem is in Class 2, but if the expressions cannot be rewritten to 
integer literals it may be more appropriate to place the result in Class 3, and would make the original
global definition of no further use.

The attributes for this are as follows:
πZ
name__FIRST == expâ1
name__LAST == expâ2
∞
The expressions in these may be rewritable to integer literals, giving the
theorems:
πZ
Ù name__FIRST == integer_literalâ1
Ù name__LAST == integer_literalâ2
∞
or similar theorems.
These theorems should be in the same class, being 1 if both expressions evaluate
to integer literals, and class 2 or 3 otherwise, and would make the original
global definitions of no further use.

The other attributes are just copies of the master type (which will be a type mark, as anonymous types are not allowed in SPARK), so the global definitions are:
πZ
name__SUCC == master__SUCC
name_PRED == master__PRED
name__POS == master__POS
name__VAL == master__VAL
∞
All of these should be in Class 1, perhaps first rewriting them with Class 1 rules for type $master$.
Notice that the domains of these attributes are those of the master types attributes.

\subsubsection{Case 2}
For 1-dimensional index constraints, noting that the constraint must be a type mark in SPARK ($index$ below), the declaration introduces global definitions for the type and some attributes:
πZ
name == {u : master | dom u = index}
∞
Note that $master$ must be an unconstrained array. 

Possible rewrites exist, such as:
πZ
name = (index ≠ comp) ° master
∞

The attributes are:
πZ
name__FIRST == index__FIRST
name__LAST == index__LAST
name__RANGE == index
∞
These are in Class 1, perhaps after having been rewritten by Class 1 theorems for $index$.

πZ
name__LENGTH == # index
∞
This is a Class 2 theorem, and it is possible that $\# index$ can be rewritten
to a simpler form introducing a 
more appropriate Class 2 theorem  (for instance $\# (n .. m)$, assuming a likely form of the underlying index type, could be rewritten to $m + 1 - n$) or even Class 1 if $\# index$ can be rewritten to an integer literal, and would make the original
global definition of no further use.

\subsubsection{Case 3}
For 2+-dimensional index constraints only a global definition of the type
is given:
πZ
name == {u : master | dom u = indexâ1 ∏ ... ∏ indexân }
∞
$u$ is an introduced bound variable, $master$ the original array type. This is in Class 2 (bordering on Class 3).
Note that $master$ must be an unconstrained array. 

This has a possible rewrite of:
πZ
name = (indexâ1 ∏ ... ∏ indexân) ≠ comp ° master
∞

\subsubsection{Case 4}
When either the constraint is not a range constraint that can be translated into $˙$, or the master type is not $˙$, or any other case not covered in the above, then
the following axiom is introduced:
πZ
name :  master
∞
This, as a membership statement, is in Class 1, perhaps normalised by other applicable Class 1 conversions and could also be used in 
resolution proof attempts.

\section{SUMMARY OF OBSERVATIONS}\label{summary}
\subsection{Conclusions of Analysis}
Almost all of the potentially useful theorems that are introduced by
a declaration are global definition paragraphs of the form:
πZ
name == expression
∞
The only potentially useful things that can be done with the global definitions at the point of declaration is:
\begin{itemize}
\item
to attempt to evaluate the expressions within \ProductZ\ and keep the resulting derived equations;
\item
add to these derived theorems any additional theorems generated from the successor or predecessor
type attribute definitions (see, e.g. the treatment of $name\_\_SUCC$ in section \ref{enumerate});
\item
make the derived theorems available to the user (e.g. save them in the theory).
Some derived theorems could be rejected here, due to categorisation, but it would 
be easier to explain the user interface if it was ``all or nothing'';
\item
to categorise the original or derived theorems and create one or more proof contexts, or
values contributing towards creating proof contexts.
\end{itemize}
The evaluation attempts could be guided by knowledge specific to the Compliance
Notation, such as the possible forms of the sub-expressions (e.g. must be 
result in ranges), or the starting
structure of the expression (e.g. one case starts as $\# typemark$).
In principle, the attempts could be guided by the name of the defined constant, but this
is vulnerable to certain (presumed rare) user actions.

The only other useful theorems introduced by global definitions
come from membership axioms that can be used in the
form:
πZ
Ù name ç expression § true
∞
which may again be used in the manners described above, or possibly as 
theorems to resolve or chain against.

\subsection{Possible Usage}
One conceivable way to apply the above analysis is to combine the pre-existing
processing of the compliance notation with building proof rules.
This is not, however, an appropriate option.
This is for the following reasons:
\begin{itemize}
\item
It interferes with the purity of critical code, that for instance must be carefully cross
checked against the specifications of the Compliance Notation specifications.
\item
The current approach of the processing is to defer introducing new Z paragraphs
until a block of compliance notation has been fully checked. 
This would make it difficult to reason about the newly introduced paragraphs
until the end of processing a block of notation, as they would not formally
exist.
\item
Forcing the mixture of specification and proof work provides a poor user interface.
\end{itemize}
 
Immediately after processing some Compliance Notation considerable additional 
contextual information is available. 
However, this information would not be available if, for instance, a user 
in a new session generated the Z paragraphs from the Z document produced by
processing some compliance notation.

This suggests that any automated support for proof work using Z paragraphs
generated as a side effect from processing the compliance notation should
be able to run purely from information in the current theory.
Furthermore, such support should correctly function even when some material was introduced
directly by the user, and should cope with previous attempts to provide support in the theory (if the support saves theorems in the database).
One particular case of user introduced material is from Z material embedded in the same
Compliance Notation script as generated the Z paragraphs we are really interested in.
The support will thus not be able to rely on determining which Z paragraphs
originate in compliance notation processing of basic declarations, nor even necessarily what sort of
compliance notation was being processed (e.g. the paragraph is from
an enumerated type declaration).

It does not seem appropriate to prevent the postulated support tools from acting
on paragraphs that are clearly not from Compliance Notation processing, but 
which can be usefully processed any provided support tools.

\section{RECOMMENDATIONS}\label{recomm}
The recommended approach to developing the ideas above and determining their usefulness is as follows:
\begin{enumerate}
\item
Create a tool (or various front ends and a single tool) that takes as input a list of axioms and definitions (or just extracts them from a nominated theory or theories) and saves theorems that are the best rewrite theorems that
can be derived using knowledge of Compliance Notation forms, as discussed in section
\ref{categ} above.
The theorems should be saved in the current theory (the user may wish to use $new\_theory$ first).
It should also bind all the derived theorems to ML value names, and produce one or more
ML bindings of lists of theorems (most importantly, all generated theorems).

This will not expand out the definitions of any constants, even those linked by apparently being generated from the same
basic declaration.

The new theorems should be sufficient that the original axioms and definitions need no longer be used 
(this ability to hide the original items is the only reason for processing axioms, as the
only relevant axioms are of the form $name : type$ which has little processing that
can be done).

\item
Explore whether any precalculated rewrites would usefully improve the overall speed of proof.
This would include considering precalculating static expressions.
This would be done in the context of the SHOLIS example material.

\item
Experiment and research to find out what, if any, forms of automatically built proof context
(or analogous automatic gathering of theorems) would be useful.
Possibilities include: 
\begin{enumerate}
\item
build only from items that concern items of type $˙$ or $ ˙$; 
\item
build only from items that concern function application or function arrows;
\item
build by Class categorisation; 
\end{enumerate}
\end{enumerate}

\newpage
\section{TOOLS}
=TEX
The following gives a size measure of a Z term, counting the number of
applications of $dest\_z\_term$ required to reach leaves.
It returns a very high number for non-Z terms.
=SML
fun size_of_z_term (tm : TERM) : int = (1 + 
	(case (dest_z_term tm) of 
		ZDec (tl,t) => ((size_of_z_term t) + 
			(fold (op +) (map size_of_z_term tl) 0)
		) | ZDecl tl => fold (op +) (map size_of_z_term tl) 0
		| ZEq (lhs, rhs) => size_of_z_term lhs + size_of_z_term rhs
		| ZTrue => 0
		| Z≥ t => size_of_z_term t
		| Z± (a, b) => size_of_z_term a + size_of_z_term b
		| Z≤ (a, b) => size_of_z_term a + size_of_z_term b
		| Z¥ (a, b) => size_of_z_term a + size_of_z_term b
		| Z§ (a, b) => size_of_z_term a + size_of_z_term b
		| Z∂ (a, b, c) => (size_of_z_term a + size_of_z_term b +
			 size_of_z_term c
		) | Z∂â1 (a, b, c) => (size_of_z_term a + size_of_z_term b +
			 size_of_z_term c
		) | Zµ (a, b, c) => (size_of_z_term a + size_of_z_term b +
			 size_of_z_term c
		) | ZSchemaPred (t, s) =>  size_of_z_term t
		| ZLVar (_, _, tl) => fold (op +) (map size_of_z_term tl) 0
		| ZInt _ => 0
		| Zß¢ (_, tl) => fold (op +) (map size_of_z_term tl) 0
		| ZSetd (_, tl) => fold (op +) (map size_of_z_term tl) 0
		| Z t => size_of_z_term t
		| ZTuple tl => fold (op +) (map size_of_z_term tl) 0
		| ZBinding tsl => fold (op +) (map (size_of_z_term o snd) tsl) 0
		| Z∏ tl => fold (op +) (map size_of_z_term tl) 0
		| Z  (t, _) => size_of_z_term t
		| ZSelâs (t, _) => size_of_z_term t
		| ZSelât (t, _) => size_of_z_term t
		| ZApp (s, t) => size_of_z_term s + size_of_z_term t
		| ZHSchema (s, t)  => size_of_z_term s + size_of_z_term t
		| ZDecorâs (t, _) => size_of_z_term t
		| Z˘âs (s, t) => size_of_z_term s + size_of_z_term t
		| ZÑâs t => size_of_z_term t
		| Zªâs (s, t) => size_of_z_term s + size_of_z_term t
		| ZRenameâs (t, _) => size_of_z_term t
		| ZSchemaDec (t, _) => size_of_z_term t
		| Zç (s, t) => size_of_z_term s + size_of_z_term t
		| ZFalse => 0
		| Z≥âs t => size_of_z_term t
		| Z±âs (s, t) => size_of_z_term s + size_of_z_term t
		| Z≤âs (s, t) => size_of_z_term s + size_of_z_term t
		| Z¥âs (s, t) => size_of_z_term s + size_of_z_term t
		| Z§âs (s, t) => size_of_z_term s + size_of_z_term t
		| Z∂âs (a, b, c) => (size_of_z_term a + size_of_z_term b +
			 size_of_z_term c
		) | Z∂â1âs (a, b, c) => (size_of_z_term a + size_of_z_term b +
			 size_of_z_term c
		) | Zµâs (a, b, c) => (size_of_z_term a + size_of_z_term b +
			 size_of_z_term c
		) | ZGVar (_, _, tl) => fold (op +) (map size_of_z_term tl) 0
		| ZString _ => 0
		| ZSeta (a, b, c) => (size_of_z_term a + size_of_z_term b +
			 size_of_z_term c
		) | ZÕ  (a, b, c) => (size_of_z_term a + size_of_z_term b +
			 size_of_z_term c
		) | ZÃ  (a, b, c) => (size_of_z_term a + size_of_z_term b +
			 size_of_z_term c
		) | ZPreâs t => size_of_z_term t
		| ZHideâs (t, _) => size_of_z_term t
		| Zòâs t => size_of_z_term t
	)
)
handle _ => 1000000;
=TEX
The following categorises a theorem according to some rough and ready rules:
=SML
fun thm_categorise (thm : THM) : int = ((
let
	val (asms, cnc) = dest_thm thm;
in
	case (dest_z_term cnc) of
=TEX
If we have an equation with a constant (without parameters) or literal, or the like on the RHS
it is Class 1 (apart from reflexive equations) - however, we distinguish
between a return value of 0 (being a literal) and 1 (being a global variable).
Otherwise, if the LHS is a global variable then the class depends on the
term size of the RHS (arbitrarily, if less than 5 then 2, less than 13 is 3, else 4).
Otherwise it is in Class 4 (a bit harsh).
=SML
	ZEq(lhs, rhs) => (if (lhs ~=$ rhs) 
		then 4
		else case (dest_z_term rhs) of
		ZInt _ => 0
		| Zß¢ (_,[]) => 0
		| ZSetd (_,[]) => 0
		| ZGVar (name,_,[]) => (case (dest_z_term lhs) of
					ZGVar (name2,_,_) => (if name = name2
						then 4
						else 1)
					| _ => 1
		)
		| ZString _ => 0
		| _ => (case (dest_z_term lhs) of
			ZGVar _ => (let val siz = size_of_z_term rhs
					in
						if siz < 5
						then 2
						else if siz < 13
						then 3
						else 4
					end
			)
			| _ => 4
		)
	)
=TEX
If something is rewriteable to $true$ or $false$ then it has to be good to use	
(though other support material is likely to canonicalise such term forms away
before categorisation).
Otherwise we are probably not interested in automatic use as the LHS 
must be complex (by the very nature of Z there are no global definitions of
names for truth values other than, as a \ProductZ\ special, $true$ and $false$.
=SML	
	| Z§(lhs, rhs) => (if (lhs ~=$ rhs) 
		then 4
		else case (dest_z_term rhs) of
		ZTrue => 0
		| ZFalse => 0
		| _ => 4
	)
=TEX
All memberships are in Class 1 - if you can rewrite a membership to $true$ it
has to be good.
=SML
	| Zç _ => 1
=TEX
The default is Class 4
=SML
	| _ => 4
end)
=TEX
Any errors will cause the result to be categorised as in Class 4
=SML
handle _ => 4);
=TEX

The following is a canonicalisation function for Compliance Notation support
work.
It is under development!
=SML
val cn_support_can : CANON = (REPEAT_CAN (FIRST_CAN [
	±_rewrite_canon,
	simple_≥_rewrite_canon,
	(fn thm => if (concl thm ~=$ ÒtrueÆ)
		then []
		else fail_canon thm
	)
	]));
=TEX
The following accumulates theorems in three lists, according to class.
The first list is Class 1, the second Class 2, the third Class 3.
Each theorem is normalised three ways, according to the previous theorems in
the three lists (it is this normalisation that prompts the lists to
be cumulative, rather than partitioned). 
If any of the normalisations result in an equation whose RHS is a literal
then the theorem is categorised as Class 1.
If normalisation by the Class 1 theorems so far produces a theorem categorised as Class 1 then it is put in that class.
Otherwise, if normalisation by the Class 1 and 2 theorems so far produces
a theorem categorised as Class 1 or Class 2 then the result is put in
Class 2.
Otherwise, if normalisation by the Class 1, 2 and 3 theorems so far produces
a theorem categorised as Class 1, 2 or 3 then the result is put in
Class 3.
Two conflicting goals are present here - categorisation by size, and 
normalisation by the same set of theorems.
We choose to place the emphasis on normalisation, because after a theorem
is used for, e.g. rewriting, from a set of Class theorems, then it
will have the normalisation applied anyway.
Thus, if a theorem,
when normalised by only Class 1 theorems so far, has a Class 2 size, 
but when normalised by the Class 2 theorems so far as well is in Class 4,
then neither normalised form will be placed in Class 2 or Class 2.
Our only exception, as noted above, is that anything normalised to equality
with a literal is placed in Class 1.

It is assumed that when a Classes theorems are used then all the theorems from the lower
Classes will also be used, but with a preference for using the highest class
first.

We cannot rely on rewriting by, e.g. a Class 2 theorem not to introduce
a potential Class 1 rewrite instance, even though the listr of Class 2 theorems
are normalised by the Class 1 theorems.
This is because we categorise theorems of the form:
$LHS = constant$ as Class 1, regardless of the form of $LHS$.
Likewise we cannot choose to only rewrite the RHS of equations, for the same reasons.
=SML
fun classify_thms ([] : THM list) 
	(lst1 : THM list) 
	(lst2 : THM list) 
	(lst3 : THM list) : 
	(THM list) * (THM list) * (THM list) = 
	(rev lst1, rev lst2, rev lst3)
| classify_thms (thm :: thmlist) lst1 lst2 lst3 = (
let 	fun local_norm (lst: THM list) (athm : THM) : THM = (
		conv_rule (pure_rewrite_conv lst) athm
		handle _ => athm
	);
	val thm1 = local_norm lst1 thm;
	val thm2 = local_norm (lst2 @ lst1) thm1;
	val thm3 = local_norm (lst3 @ lst2 @ lst1) thm2;
	val cat1 = thm_categorise thm1;
	val cat2 = thm_categorise thm2;
	val cat3 = thm_categorise thm3;
	val thms1 = if (cat3 = 0) then [thm3]
			else if (cat2 = 0) then [thm2]
			else if (cat1 = 1 orelse cat1 = 0) then [thm1]	
			else [];
	val thms2 = if (cat2 = 2 orelse cat2 = 1 andalso
                        (is_nil thms1 orelse not(hd thms1 ~=|- thm2))) 
			then [thm2]
			else [];
	val thms3 = if (cat3 = 3 orelse cat3 = 2 orelse cat3 = 1 andalso
                        (is_nil thms1 orelse not(hd thms1 ~=|- thm3)) andalso
                        (is_nil thms2 orelse not(hd thms2 ~=|- thm3))) 
			then [thm3]
			else [];	
in
	classify_thms thmlist (union (op ~=|-) thms1 lst1) 
		(union (op ~=|-) thms2 lst2) (union (op ~=|-) thms3 lst3)
end);	
	
=TEX
The following should produce three lists of conversions (currently theorems), for
Classes 1, 2 and 3 respectively.
=SML
fun cn_support_cnvs (thy : string)  = (
let
	val I = Combinators.I;
	val can_list = (fn lst =>
			flat (map cn_support_can lst));
=TEX
We use $z\_para\_pred\_conv$ to convert, as far as possible, Z paragraph
forms as they are stored in \ProductZ\ into Z theorems.
$get\_defns$ and $get\_axioms$ return lists in the reverse order of introduction:
we wish to process them the other way round.

Note that in, say, the list for Class 3, we will wish to apply theorems from
Class 3 in preference to those for Class 2 or 1.
=SML
	val zdefns = rev (map (conv_rule (TRY_C z_para_pred_conv) o snd)
		 (get_defns thy));
	val zaxs = rev(map (conv_rule (TRY_C z_para_pred_conv) o snd)
		 (get_axioms thy));
	val zdefns1 = can_list zdefns;
	val zaxs1 = can_list zaxs;
	val (lst1, lst2, lst3) = classify_thms (zdefns1 @ zaxs1) [] [] [];
in
	(lst1, lst2, lst3)
end);

=TEX

=TEX
\end{document}
