=IGN
********************************************************************************
wrk509.doc: this file is part of the PPDaz system

Copyright (c) 2002 Lemma 1 Ltd.

See the file LICENSE for your rights to use and change this file.

Contact: Rob Arthan < rda@lemma-one.com >
********************************************************************************
% %Z% $Date: 2002/10/17 16:04:45 $ $Revision: 1.14 $ $RCSfile: wrk509.doc,v $
=TEX
% TQtemplate.tex
% use_file "wrk509";
% doctex wrk509x[0-9][0-9].th
% docdvi wrk509
\documentstyle[hol1,11pt,TQ,hyperbasics]{article}
\ftlinepenalty=9999
\def\Hide#1{}
\def\Bool{``$\it{:}bool\,$''}
\makeindex
\TPPproject{DAZ PROJECT}  %% Mandatory field
%\TPPvolume{}
%\TPPpart{}
\TPPtitle{Analysis of Declarations}  %% Mandatory field
\TPPref{ISS/HAT/DAZ/WRK509}  %% Mandatory field
\def\SCCSversion{$Revision: 1.14 $%
}
\TPPissue{\SCCSversion}  %% Mandatory field
\TPPdate{\FormatDate{$Date: 2002/10/17 16:04:45 $%
}}  %% Mandatory field (with sensible default)
\TPPstatus{Draft}
%\TPPstatus{Informal}
\TPPtype{Technical}
%\TPPkeywords{HOL}
\TPPauthor{K.~Blackburn & WIN01}  %% Mandatory field
%\TPPauthors{Name 1&location 1\\Name 2&location 2\\Name 3&location 3}
\TPPauthorisation{R.D.~Arthan & HAT Team}
\TPPabstract{%
As part of the requirements analysis for proof tools for the Compliance Notation, the generation of Z paragraphs for basic declarations has been analysed.
This paper reports on that analysis.}
%\TPPabstractB{}
%\TPPabstractC{}
%\TPPabstractD{}
%\TPPabstractE{}
%\TPPabstractF{}
\TPPdistribution{\parbox[t]{4.0in}{%
      Library}}

%\TPPclass{CLASSIFICATION}
\def\TPPheadlhs{Lemma 1 Ltd.}
%\def\TPPheadlhs{}
%\def\TPPheadcentre{}
%def\TPPheadrhs{}
%\def\TPPfootlhs{}
%\def\TPPfootcentre{}
%\def\TPPfootrhs{}

\begin{document}
\TPPsetsizes
\makeTPPfrontpage

\vfill
\begin{centering}

\bf Copyright \copyright\ : Lemma 1 Ltd. \number\year

\end{centering}

\newpage
\section{DOCUMENT CONTROL}
\subsection{Contents List}
\tableofcontents
%\subsection{Document Cross References}
\bibliographystyle{fmu}
\bibliography{fmu,daz}
\subsection{Changes History}  % to get section number `0.3'
\begin{description}

\item[Issue 1.1-1.2] Initial Drafts.
\item[Issue 1.3]
Added thoughts on possible usage.
\item[Issue 1.4-1.7]
Extensive changes after RDA comments.
\item[Issue 1.8]
Changed as called for by review 26 of \cite{ISS/HAT/DAZ/PLN005},
and added an alternative way of viewing theorems as section \ref{alternative},
and a change to the recommendations to reflect this.
\item[Issue 1.9]
Follow up work to review.
\item[Issue 1.10]
Updated references.
\item[Issue 1.11] Removed use of ICL logo font.
\item[Issue 1.12] Copyright and banner updates for open source release.
\item[Issue 1.13] DAZ-specific updates to banner for open source release
\item[Issue 1.14] DAZ-specific updates to banner for open source release
\item[Issue 1.15] The Z universal set is now called É.
\end{description}
\subsection{Changes Forecast}
As called for by review by DRA and ICL.
\pagebreak
\section{GENERAL}
\subsection{Scope}
This document reports on a study called for in \cite{ISS/HAT/DAZ/PLN010,ISS/HAT/DAZ/PLN012}.
\subsection{Introduction}
As part of the requirements analysis for proof tools for the Compliance Notation, the generation of Z paragraphs for basic declarations has been analysed.
This paper reports on that analysis.

The approach we take is to consider the different categories of basic declaration syntax
on a case by case basis, and then draw together these results.
This follows the categories given in \cite{DRA/CIS/CSE3/TR/94/27/3.0}.

Thus this document is organised as follows:
\begin{itemize}
\item
in section \ref{categ} we analyse each of the forms of basic declaration by category;
\item
in section \ref{summary} we summarise the findings of the previous section, and consider the possible contexts when using the analysis;
\item
and in section \ref{recomm} we recommend what approaches should be taken to provide proof support
for this area of Compliance Tools Notation processing.
\end{itemize}


\section{CATEGORIES OF BASIC DECLARATIONS}\label{categ}

Before we can consider the categories of basic declarations in a case by case
approach, we need some introductory material.
Thus we start with some notation that we will use in the rest of the section, given in sub-section \ref{Notation}.
We then give some general observations that apply to the analysis of many of the
categories of Basic Declaration, in sub-section \ref{genobs}.
Then sub-sections \ref{firstcat} to \ref{lastcat} deal with each type of declaration in turn.

\subsection{Notation}\label{Notation}

\subsubsection{Pseudo-Z}
In the following discussions we use $name$ to indicate the name of a Z constant, such
as a type name or an enumerated item, subscripting $name$ if necessary to remove ambiguity.
We use $integer\_literal$ to indicate an integer literal, perhaps generated from
static evaluation of an expression.
We use the Compliance Tool's notation connective, namely ``v'', to name the attributes of a type,
by connecting $name$ to the attribute suffix.

The results of this are sometimes not Z, but are meant to highlight the structure of theorems, etc., for the purpose of discussing the usefulness of the items for rewriting.

\subsubsection{A Hierarchy of Usefulness}\label{hierarchy}
The definitions and axioms generated during the processing of Compliance Tool Notation
will presumably all play a part in a proof concerning the items defined or axiomatised.
However, it would be very slow, and likely to ``explode'' goals into being unmanageably large if we always rewrote with all of these theorems.
What would be useful, therefore, is a way of categorising theorems as to when we should use them
for rewriting.

Having seen the need for such categorisation, we can generalise the concept, to consider not just rewriting with theorems, but the application of any conversion to a term (where
the application of a rewriting conversion using a theorem is just one possibility).
The word ``conversion'' is \Product\ terminology for an inference rule which takes
as an argument a term $t$ and returns a theorem with a conclusion of the form
$t = t'$.

One can do the categorisation based on a specific application.
For instance, if one was concerned with determining the Z type of function applications, then the
only rewrites of interest would be ones dealing with function arrows or function applications.
Alternatively, categorisation can be based on the complexity of the of the result of the conversion (or of the rewriting), and this could be done as follows:

A conversion can be categorised as to its likely usefulness by its complexity as follows.
\begin{description}
\item[Class 1]
Appropriate to be a conversion for the default rewrite rules, etc., in a straightforward proof context in the appropriate theory.

Such a conversion should simplify the term it is applied to, and not lengthen it (by count of tokens).
Appropriate conversions for this class would for example be ones that replace constants by literals.

It is still possible that a user will not want the conversion applied in all cases, but only with the sort of frequency that, for instance pure rewriting is chosen over rewriting with the current proof context rewrite rules.

\item[Class 2]
Appropriate to be a conversion used within a straightforward automatic proof procedure (e.g. the one for $prove\_tac$ in a straightforward proof context), but not one that should ``always'' be applied.
A user would probably want easy access to such a conversion.

Such a conversion should simplify (in some sense) the term it is applied to, but may lengthen it to some extent.
Appropriate conversions would be ones that move the proof into a ``lower'' theory,
such as rewriting a Compliance Notation expression into a term only using operators from the theory of relations, when reasoning about the Compliance Notation form of the term is unlikely to progress any other way.

\item[Class 3]
Appropriate only to automatic proof tools that take a ``brutal'' approach to
a proof, for instance,  conversion that simplifies, but greatly lengthens, a term, whose output is not intended to be human readable.
Such a conversion will not often be of use in a manual proof.

Some of the extensionality conversions would fit in this class.

\item[Class 4]
A conversion that is has no appropriate use except for careful manual use.

This includes many recursive definitions and conversions that reverse the effect of
conversions from other classes.
\end{description}


\subsubsection{Type Marks}
Note that the specification of SPARK uses ``type mark'' for ``type name or subtype name'',
and this wording is used in this document.
In SPARK all syntax (other than type definitions) that requires a type demands a type mark,
i.e. a name that will be associated with a definition.
That is, SPARK does not allow anonymous types.
This simplifies the variety of possible syntax to be considered.

\subsection{General Observations}\label{genobs}
\subsubsection{Static Expressions}
Various expression slots in declarations must be given static expressions in SPARK.
It should be noted that this does not necessarily mean that they can be evaluated
within \ProductZ.
This is for a variety of reasons, such as the lack of an axiomatisation of SPARK
reals in the Compliance Tool.
Such reasons normally manifest by some constituent of the formal expansion of such a
value being axiomatically introduced as a signature without an actual definition (i.e. as how a real number would be handled).
However, any expression that is meant to be statically evaluated to a $˙$ value
is assumed to be a good candidate for a ``once and for all'' attempt at
evaluation in \ProductZ, rather than being left for repeated attempts during
individual proofs.

The may also be problems handling predecessor and successor functions in static evaluations
carried out under \ProductZ.

\subsubsection{Ranges Versus Set Displays}
The Compliance Tool works in a context where we have access to some
moderately powerful linear arithmetic tools.
In particular these tools are capable of reasoning about the inequalities that can be
derived from a range expression, but will be inefficient when using the results of
expanding ranges into set displays (which will give predicates of the form $x = n ≤ x = n+1 ≤ ...$).
This suggests that, in our area of particular interest, range expressions are preferable even to a two element set display of numbers.

\subsection{Variables}\label{firstcat}
Variable declarations do not cause the introduction of Z
paragraphs, and are therefore not further considered in this document.

\subsection{Constants}
πCN
name : constant name_type := exp;
∞
The SPARK specification and the actual \ProductZ\ implementation allows multiple constant identifiers to be
introduced in one statement, but this is not covered in the Compliance Notation
specifications (they are implicitly handled as multiple individual declarations).
$exp$ must be a static expression in SPARK.

A declaration of a constant will either introduce a global definition or an axiom, depending on whether the constant's expression can be translated into Z.

These Z paragraphs are either of the form:
πZ
name == exp
∞
or:
πZ
name : name_type
∞

A theorem providing the rewrite of the name to the best available evaluation of the expression in the first case
would of use (for it must be a static expression, so evaluation within \ProductZ\ is at least possible).
Such a theorem would be in Class 1 if the expression can be evaluated to a literal or other previously defined constant, and otherwise would probably fall in class 2 (and the worst cases in Class 3), and would make the original
global definition of no further use.

There is no obvious processing for the second case, but the axiom (used as a membership statement of the form $name ç type § true$, perhaps normalised with other Class 1  conversions) is in Class 1.

\subsection{Enumeration Types}\label{enumerate}
πCN
type name is (nameâ0, ..., nameân)
∞
There must be at least 2 enumeration literals.

The introduction of an enumeration type introduces definitions of the enumeration literals, the type itself, and the type's attributes.
In more detail:

There will be one global definition per enumeration literal, of the form:
πZ
name == integer_literal
∞
Such theorems are in class 1.

The global definition of the type is of the form:
πZ
name == nameâ0 .. nameân
∞
This can be rewritten to:
πZ
Ù name = 0 .. integer_literal
∞
or various other obvious forms, e.g. involving inequalities, or simplifications
if there are few enumerated elements.
The integer literal will be one less than the number of enumerated items.
This rewritten theorem belongs in Class 2, and would make the original
global definition of no further use.

The attributes are as follows:

πZ
namevFIRST == nameâ0
namevLAST == nameân
∞
which can be rewritten to:
πZ
Ù namevFIRST = 0
Ù namevLAST = integer_literal
∞
These rewritten theorems belong in Class 1, and would make the original
global definitions of no further use.
The integer literal will be one less than the number of enumerated items.

πZ
namevSUCC == (name \ {namevLAST}) Ú succ
∞
This has a variety of possible rewrites. The two extremes might be:
πZ
Ù namevSUCC == (0 .. integer_literal) Ú succ
Ù namevSUCC == {(0,1), ..., (integer_literalËn-1Í, integer_literalËnÍ)}
∞
In general, the first belongs in Class 2, the second in Class 3.
However, if the enumerated type is small (2, maybe 3, elements), then
then simplifications can be made to the first rewritten theorem, and the results moved up the classes.
Any of these would make the original
global definition of no further use.
Some rewriting use may be made of the fact that $succ$ is already domain restricted to $Ó$.

We perhaps more usefully could introduce the theorem:
πZ
Ù µ i : 0 .. integer_literal ∑ namevSUCC i = i + 1
∞
where $integer\_literal$ was the number of enumerated items minus 2.

πZ
namevPRED == ((name \ {namevLAST}) Ú succ)~
∞
This has a variety of possible rewrites. The two extremes might be:
πZ
Ù namevPRED == (1 .. integer_literal) Ú (succ ~)
Ù namevPRED == {(1,0), ..., (integer_literalân, integer_literalËn-1Í)}
∞
In general, the first belongs in Class 2, the second in Class 3.
However, if the enumerated type is small (2, maybe 3, elements), then
then simplifications can be made to the first rewritten theorem, and both the rewritten theorem and the second global definition moved up the classes.
Any of these would make the original
global definition of no further use.
Some rewriting use may be made of the fact that the range of $succ$ is restricted to $Óâ1$.

We perhaps more usefully could introduce the theorem:
πZ
Ù µ i : 1 .. integer_literal ∑ namevPRED i = i - 1
∞
where $integer\_literal$ was the number of enumerated items minus 1.

πZ
namevPOS == id name
namevVAL == namevPOS~
∞
The second can be rewritten to:
πZ
Ù namevVAL = namevPOS
∞
The rewritten theorem should probably be in Class 1, the first global definition should be in Class 2 (it is too expansive, given the follow-on rewrite of $name$, to be in Class 1), and would make the original
global definition of no further use.
A rewrite theorem of the following form might be put in Class 3 if the type is small (if very small, perhaps even in Class 2):
πZ
Ù namevPOS = {(0,0),...(integer_literalân, integer_literalân)}
∞

We perhaps more usefully could introduce the theorems:
πZ
Ù µ i : 0 .. integer_literal ∑ namevPOS i = i
Ù µ i : 0 .. integer_literal ∑ namevVAL i = i
∞
where $integer\_literal$ was the number of enumerated items minus 1.

\subsection{Array Types}
Constrained array type declarations are of the form:
πCN
type name is array (indexâ1, ..., indexân) of comp;
∞
The indexes must be type marks for discrete, and thus static, ranges.

The introduction of a constrained array type with component type mark $comp$ will introduce global definitions for the type itself and, if only 1-dimensional, its attributes.

πZ
name == indexâ1 ∏ ... ∏ indexân ≠ comp
∞
This definition is in Class 2 or maybe even 3, depending on the number and character of the index types.

For 1-dimensional arrays the attribute definitions are:
πZ
namevFIRST == indexâ1vFIRST
namevLAST == indexâ1vLAST
namevLENGTH == # indexâ1
namevRANGE == indexâ1
∞
All but the 3rd of these are in Class 1, the 3rd one being in Class 2.
It is possible that $\# indexâ1$ can be rewritten to a simpler value, maybe even an
integer literal (for instance $\# (n .. m)$, assuming a likely form of the underlying index type, could be rewritten to $m + 1 - n$).
This may move the rewritten theorem to Class 1, and would make the original
global definition of no further use.

Unconstrained array type declarations are of the form:
πCN
type name is array (indexâ1 range <>, ..., indexân range <>) of comp;
∞
The indexes are arbitrary type marks, apparently, and in particular do not
appear to need to be static in SPARK.

The introduction of an unconstrained array type only introduces an axiom of the form:
πZ
name :  (indexâ1 ∏ ... ∏ indexân ﬂ comp)
∞
This axiom, used as a membership statement, perhaps normalised by applying other applicable Class 1 conversions, is probably in Class 1, and could also be used in
resolution proof attempts.

\subsection{Record Types}
πCN
type name is
record
	labâ1 : ltypeâ1;
	...
	labân : ltypeân;
end record;
∞
The Compliance Notation specification does not address the issue of multiple
identifiers in a single component declaration,
but the Compliance Tool does treat such things tidily, preserving the use of a single component declaration for multiple names.
The $ltype$s are all type marks.

The introduction of a record type will introduce the following style of schema:
ˇ name ¸¸¸¸¸¸¸¸¸¸¸
‹ labâ1 : ltypeâ1
‹ ...
‹ labân : ltypeân
à¸¸¸¸¸¸¸¸¸¸¸¸¸¸
but as a horizontal schema, i.e.
πZ
name == [labâ1 : ltype; ...; labân : ltypeân]
∞
This equation of itself is probably in Class 2 or 3.
It is also something that perhaps ought to be rewritten with in some circumstances,
regardless of the size of the horizontal schema, as it is crucial for progress.
It can also be used to generate various type membership theorems for the projections, that might be
useful (and thus perhaps even in Class 1 due to their simple nature).

\subsection{Integer Types}
πCN
type name is expâ1 .. expâ2
∞
(range attributes are valid SPARC range constraints, but are not handled formally)

Integer type declarations are types with the range constraint being statically determinable.
Their introduction causes the introduction of a global definition of themselves and either global definitions or axioms for their attributes.

πZ
name == expâ1 .. expâ2
∞
This is a Class 2 theorem, and there may be possible rewrites of it, as
the two expressions should be static.

πZ
namevFIRST == expâ1
namevLAST == expâ2
∞
Rewritten theorems based on the evaluation of the expressions should be in Class 1 if the results are integer literals or other constants, and Class 2 otherwise, and would make the original
global definitions of no further use.

πZ
namevSUCC : ˙ ﬂ ˙
namevPRED : ˙ ﬂ ˙
namevPOS : ˙ ﬂ ˙
namevVAL : ˙ ﬂ ˙
∞
These, taken as membership statements are in Class 1.
The translation of the Compliance Notation does not give the definitions for
these constants, only axioms giving just a signature.
This is to do with worries about overflows.

\subsection{Real Types}
πCN
type name is delta ...
type name is digits ...
∞

The introduction of a real type introduces the type as a given set:
πZ
[ name ]
∞
This may be used to rewrite any predicate of the form $r ç name$ to $true$.
\Product\ has a standard approach to this so-called É-elimination.

\subsection{Subtypes}\label{lastcat}
πCN
subtype name is master constraint
∞
The constraint can be range, floating point, fixed point or index, and, from SPARK, must
be statically determinable.

There are various possibilities for subtypes, depending on whether or not they are subtypes of types that are translated into $˙$, and on the nature of their constraint (e.g. can it be translated into a range constraint).

\subsubsection{Case 1}
If the master type, $master$, can be translated to $˙$ and the constraint can be translated to a $˙$ range constraint $expâ1 .. expâ2$ global definitions are introduced for the type and its attributes:
πZ
name == expâ1 .. expâ2
∞
The expressions must be static, and thus we can hope to be able to
rewrite this to something of the form:
πZ
Ù name = integer_literalâ1 .. integer_literalâ2
∞
Such a theorem is in Class 2, but if the expressions cannot be rewritten to
integer literals it may be more appropriate to place the result in Class 3, and would make the original
global definition of no further use.

The attributes for this are as follows:
πZ
namevFIRST == expâ1
namevLAST == expâ2
∞
The expressions in these may be rewritable to integer literals, giving the
theorems:
πZ
Ù namevFIRST == integer_literalâ1
Ù namevLAST == integer_literalâ2
∞
or similar theorems.
These theorems should be in the same class, being 1 if both expressions evaluate
to integer literals, and class 2 or 3 otherwise, and would make the original
global definitions of no further use.

The other attributes are just copies of the master type (which will be a type mark, as anonymous types are not allowed in SPARK), so the global definitions are:
πZ
namevSUCC == mastervSUCC
name_PRED == mastervPRED
namevPOS == mastervPOS
namevVAL == mastervVAL
∞
All of these should be in Class 1, perhaps first rewriting them with Class 1 rules for type $master$.
Notice that the domains of these attributes are those of the master types attributes.

\subsubsection{Case 2}
For 1-dimensional index constraints, noting that the constraint must be a type mark in SPARK ($index$ below), the declaration introduces global definitions for the type and some attributes:
πZ
name == {u : master | dom u = index}
∞
Note that $master$ must be an unconstrained array.

Possible rewrites exist, such as:
πZ
name = (index ≠ comp) ° master
∞

The attributes are:
πZ
namevFIRST == indexvFIRST
namevLAST == indexvLAST
namevRANGE == index
∞
These are in Class 1, perhaps after having been rewritten by Class 1 theorems for $index$.

πZ
namevLENGTH == # index
∞
This is a Class 2 theorem, and it is possible that $\# index$ can be rewritten
to a simpler form introducing a
more appropriate Class 2 theorem  (for instance $\# (n .. m)$, assuming a likely form of the underlying index type, could be rewritten to $m + 1 - n$) or even Class 1 if $\# index$ can be rewritten to an integer literal, and would make the original
global definition of no further use.

\subsubsection{Case 3}
For 2+-dimensional index constraints only a global definition of the type
is given:
πZ
name == {u : master | dom u = indexâ1 ∏ ... ∏ indexân }
∞
$u$ is an introduced bound variable, $master$ the original array type. This is in Class 2 (bordering on Class 3).
Note that $master$ must be an unconstrained array.

This has a possible rewrite of:
πZ
name = (indexâ1 ∏ ... ∏ indexân) ≠ comp ° master
∞

\subsubsection{Case 4}
When either the constraint is not a range constraint that can be translated into $˙$, or the master type is not $˙$, or any other case not covered in the above, then
the following axiom is introduced:
πZ
name :  master
∞
This, as a membership statement, is in Class 1, perhaps normalised by other applicable Class 1 conversions and could also be used in
resolution proof attempts.

\section{SUMMARY OF OBSERVATIONS}\label{summary}
\subsection{Conclusions of Analysis}
Almost all of the potentially useful theorems that are introduced by
a declaration are global definition paragraphs of the form:
πZ
name == expression
∞
Some useful things that can be done with the global definitions at the point of declaration are:
\begin{itemize}
\item
to attempt to evaluate the expressions within \ProductZ\ and keep the resulting derived equations;
\item
add to these derived theorems any additional theorems generated from the successor or predecessor
type attribute definitions (see, e.g. the treatment of $namevSUCC$ in section \ref{enumerate});
\item
make the derived theorems available to the user (e.g. save them in the theory).
Some derived theorems could be rejected here, due to categorisation, but it would
be easier to explain the user interface if it was ``all or nothing'';
\item
to categorise the original or derived theorems and create one or more proof contexts, or
values contributing towards creating proof contexts.
\end{itemize}
The evaluation attempts could be guided by knowledge specific to the Compliance
Notation, such as the possible forms of the sub-expressions (e.g. must be
result in ranges), or the starting
structure of the expression (e.g. one case starts as $\# typemark$).
In principle, the attempts could be guided by the name of the defined constant, but this
is vulnerable to certain (presumed rare) user actions.

The only other useful theorems introduced by global definitions
come from membership axioms that can be used in the
form:
πZ
Ù name ç expression § true
∞
which may again be used in the manners described above, or possibly as
theorems to resolve or chain against.

\subsubsection{An Alternative Categorisation}\label{alternative}
When considering how to use definitions introduced by processing the Compliance
Notation we can use the categorisation
into Classes 1 to 4 of section \ref{hierarchy}.
However, another way we can categorise, and then process, definitions is as follows:
\begin{itemize}
\item
The definition is of a constant.
\item
The definition is of a kind of type attribute that is always a function
(e.g. the successor attribute).
Such definitions can be presented as a conditional rewrite for the
application of the function, perhaps in combination with a type membership statement.
\item
The definition is of a kind of type attribute that is not always a function (e.g. scalar or a range).
\item
The definition equates a type to a set abstraction of a standard form for the
Compliance Notation (e.g. cases 2 and 3 of subtypes).
Such definitions can be presented as defining the type equal to the intersection of a total function
and some pre-existing type, which may be more convenient for reasoning.
\item
The definition is of a type, but is some other form than one of the standard set abstraction forms.
\end{itemize}
We can only derive the two kinds of alternative presentation noted if we use the definitions
of other Compliance Notation definitions to simplify the definition of interest.

\subsection{Possible Usage}
One conceivable way to apply the above analysis is to combine the pre-existing
processing of the compliance notation with building proof rules.
This is not, however, an appropriate option.
This is for the following reasons:
\begin{itemize}
\item
It interferes with the purity of critical code, that for instance must be carefully cross
checked against the specifications of the Compliance Notation specifications.
\item
The current approach of the processing is to defer introducing new Z paragraphs
until a block of compliance notation has been fully checked.
This would make it difficult to reason about the newly introduced paragraphs
until the end of processing a block of notation, as they would not formally
exist.
\item
Forcing the mixture of specification and proof work provides a poor user interface.
\end{itemize}

Immediately after processing some Compliance Notation considerable additional
contextual information is available.
However, this information would not be available if, for instance, a user
in a new session generated the Z paragraphs from the Z document produced by
processing some compliance notation.

This suggests that any automated support for proof work using Z paragraphs
generated as a side effect from processing the compliance notation should
be able to run purely from information in the current theory.
Furthermore, such support should correctly function even when some material was introduced
directly by the user, and should cope with previous attempts to provide support in the theory (if the support saves theorems in the database).
One particular case of user introduced material is from Z material embedded in the same
Compliance Notation script as generated the Z paragraphs we are really interested in.
The support will thus not be able to rely on determining which Z paragraphs
originate in compliance notation processing of basic declarations, nor even necessarily what sort of
compliance notation was being processed (e.g. the paragraph is from
an enumerated type declaration).

It does not seem appropriate to prevent the postulated support tools from acting
on paragraphs that are clearly not from Compliance Notation processing, but
which can be usefully processed any provided support tools.

\subsection{Other Z Paragraph Generating Parts of the Compliance Notation}
Some of the conclusions reached above, from considering all the possibilities for
Basic Declarations, can be applied to the other areas of the Compliance Notation that
cause Z paragraphs to be generated (we are not concerned with Z embedded in the Notation
by the user).
The only instance of generating paragraphs
(other than conjectures) is the case of function declarations.

\section{RECOMMENDATIONS}\label{recomm}
The recommended approach to developing the ideas above and determining their usefulness is as follows:
\begin{enumerate}
\item
Create a tool (or various front ends and a single tool) that takes as input a list of axioms and definitions (or just extracts them from a nominated theory or theories) and saves theorems that are the best rewrite theorems that
can be derived using knowledge of Compliance Notation forms, as discussed in section
\ref{categ} above.
The theorems should be saved in the current theory (the user may wish to use $new\_theory$ first).
It should also bind all the derived theorems to ML value names, and produce one or more
ML bindings of lists of theorems (most importantly, all generated theorems).

This will not expand out the definitions of any constants, even those linked by apparently being generated from the same
basic declaration.

The new theorems should be sufficient that the original axioms and definitions need no longer be used
(this ability to hide the original items is the only reason for processing axioms, as the
only relevant axioms are of the form $name : type$ which has little processing that
can be done).

\item
As a variation on the above, create a similar set of tools, except that
the alternative presentations discussed in section \ref{alternative} will be
used.
In these alternative presentations, and only in these cases, will we drop the
rule that processed definitions will not be simplified using other definitions generated
by the compliance tool.

\item
Explore whether any precalculated rewrites would usefully improve the overall speed of proof.
This would include considering precalculating static expressions.
This would be done in the context of the SHOLIS example material.

\item
Experiment and research to find out what, if any, forms of automatically built proof context
(or analogous automatic gathering of theorems) would be useful.
Possibilities include:
\begin{enumerate}
\item
build only from items that concern items of type $˙$ or $ ˙$;
\item
build only from items that concern function application or function arrows;
\item
build by Class categorisation;
\end{enumerate}
\end{enumerate}
\end{document}
